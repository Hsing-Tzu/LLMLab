{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e8bb9c",
   "metadata": {},
   "source": [
    "# 在Colab使用TPU Fine-tuning GPT模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94570600",
   "metadata": {},
   "source": [
    "## 環境準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0b2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cloud-tpu-client\n",
    "curl -O https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py\n",
    "python env-setup.py --version 1.12 --apt-packages libomp5 libopenblas-dev\n",
    "pip install torch torchvision\n",
    "pip install cloud-tpu-client torch-xla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33ea47",
   "metadata": {},
   "source": [
    "## Fine-tuning 過程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length=512):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer(txt, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "            self.labels.append(torch.tensor(encodings_dict['input_ids']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.input_ids[idx], 'attention_mask': self.attn_masks[idx], 'labels': self.labels[idx]}\n",
    "\n",
    "# You may need to change the data to the real training data.\n",
    "data_list = [\"This is the first sample from Kenny.\", \"This is the second sample from Jessica.\", \"This is the third sample from Mason.\"]\n",
    "\n",
    "my_dataset = MyDataset(data_list, tokenizer)\n",
    "train_loader = DataLoader(my_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "def train_model():\n",
    "    device = xm.xla_device()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    num_epochs = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        para_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
    "        for batch in para_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            xm.optimizer_step(optimizer)\n",
    "\n",
    "            print(f\"Epoch {epoch} | Loss: {loss.item()}\")\n",
    "\n",
    "            xm.mark_step()\n",
    "\n",
    "    if xm.is_master_ordinal():\n",
    "        model.to('cpu')\n",
    "\n",
    "        model.save_pretrained('/content/drive/MyDrive/model_output')\n",
    "\n",
    "def _mp_fn(rank, flags):\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    train_model()\n",
    "\n",
    "FLAGS = {}\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c401afa9",
   "metadata": {},
   "source": [
    "## 實測Fine-tuning過後的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = '/content/drive/MyDrive/model_output'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ccd891",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '''What is the capital of France?'''\n",
    "\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "output = model.generate(input_ids, max_length=150, num_return_sequences=1)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291235e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
