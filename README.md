# LLM LAB
**Members**
@Hsing-Tzu @kennyliou @imjessica @mason45ok

https://colab.research.google.com/drive/1XMsbNs6XIOiWrGhxLqI09H7xzjVWqvQl?usp=sharing

https://meet.google.com/bxo-ysue-ezs

**Chat**
I am Daisy. I am so beautiful.
Today Kenny teacher
So we can start now 
Awesome!!!!!

**11/2çªç„¶æƒ³åˆ°**
> [name=åŠ‰å½¥è°·]
å¦‚æœpythonçš„è¼¸å‡ºæ˜¯ä½¿ç”¨è€…ç„¡æ³•é æ¸¬ï¼Œä¾‹å¦‚çˆ¬èŸ²å›è¦†çš„çµæœæˆ‘å€‘ç„¡æ³•é æ¸¬PTTçš„æ‰€æœ‰æ¨™é¡Œï¼Œé‚£æˆ‘å€‘å°±åœ¨å¾—åˆ°ç¬¬ä¸€æ¬¡API_fetchæ™‚ï¼Œå°å‡ºåŸ·è¡Œçµæœè®“ä½¿ç”¨è€…å›è¦†æ˜¯æˆ–å¦ï¼Œä»¥åˆ¤æ–·error = 1 or error = 0ã€‚

**11/2çªç„¶æƒ³åˆ° Part 2**
> [name=åŠ‰å½¥è°·]
æˆ‘å€‘ç¨‹å¼çš„éç¨‹æ˜¯ä¸æ˜¯å°±æ˜¯ä¸€å€‹fine-tuneçš„éç¨‹ï¼Œé‚£æˆ‘å€‘æ˜¯ä¸æ˜¯å¯ä»¥åŒæ™‚æŠŠjsonç”Ÿå‡ºä¾†

## 11/1ï½œFirst Discussion
### Topic 
è§£æ±ºè‡ªå·±åœ¨å­¸ç¿’Pythonæ™‚ï¼Œç•¶æˆ‘å€‘è©¢å•ChatGPTç›¸é—œå•é¡Œæ™‚ï¼Œæœƒå¾—åˆ°ä¸æº–ç¢ºçš„ç­”æ¡ˆï¼Œæˆ‘å€‘å¸Œæœ›èƒ½ä½¿å…¶ç­”å‡ºæº–ç¢ºçš„ç­”æ¡ˆï¼Œå› æ­¤ï¼Œæˆ‘å€‘å¸Œæœ›ChatGPTç”¢å‡ºç¨‹å¼ç¢¼ä¹‹å¾Œï¼Œå¯ä»¥å…ˆç·¨è­¯éï¼Œç¢ºèªéåŸ·è¡Œçµæœæ˜¯æ­£ç¢ºçš„ï¼Œå†å›å‚³ç¨‹å¼ç¢¼ã€‚
### Function
* API fetch
* jsonè™•ç†
    * å°‹æ‰¾å…©å°ä¸‰å€‹ã€Œ ` ã€æ‰€åŒ…å«çš„ç¨‹å¼ç¢¼ï¼Œä¸¦ä¸”åˆªé™¤ä¸å¿…è¦çš„ç¬¦è™Ÿ
* ç·¨è­¯
    * eval
* æª¢æŸ¥
    * eval çµæœæ˜¯å¦ç­‰æ–¼æ¸¬è³‡ï¼Œæ˜¯ï¼šå›å‚³pythonã€å¦ï¼šé‡è¤‡åŸ·è¡Œ
### ç¨‹å¼åŸ·è¡Œæµç¨‹
#### é€™æ˜¯ä¸€å€‹è˜‡éƒ½æ‰£
situation = 0;
```python=
while True:
    if situation == 1:
        #call API
    
    #è£½ä½œä¸€å€‹test.py
    #åŸ·è¡Œtest.py
    #æ¥æ”¶å›å‚³çµæœ
```
#### å·¥å…·
```python=
# è¨­å®šè¦åŸ·è¡Œçš„Pythonè…³æœ¬æ–‡ä»¶
command = "python try.txt"

# ä½¿ç”¨subprocessåŸ·è¡Œå‘½ä»¤
process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

# ç­‰å¾…å‘½ä»¤å®Œæˆ
stdout, stderr = process.communicate()

# æª¢æŸ¥å‘½ä»¤æ˜¯å¦æˆåŠŸ
if process.returncode == 0:
    print("å‘½ä»¤æˆåŠŸåŸ·è¡Œï¼Œè¼¸å‡ºç‚ºï¼š")
    print(stdout)
else:
    print("å‘½ä»¤åŸ·è¡Œå¤±æ•—ï¼ŒéŒ¯èª¤è¨Šæ¯ç‚ºï¼š")
    print(stderr)

```
å¯«å…¥GPTè¼¸å‡ºè‡³txt
```python=
user_result = '''
#python 
from bs4 import BeautifulSoup as bs
import requests

url="https://www.ptt.cc/bbs/Stock/index6651.html"
headers={
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
}

res = requests.get(url , headers=headers)
print(res.text)'''
path = 'python try'
f = open(path, 'w')
f.write(str(user_result))
f.close()
```
### æœ€çµ‚ç¨‹å¼ç¢¼
https://github.com/Hsing-Tzu/LLMLab/blob/main/LLMLAB_1101.ipynb
```python
user_result = "Hello World"
user_question = "1Can you give me the python code which can print Hello World?"

# prompt = ""+ user_question + "" + "ï¼Œä¸”é æœŸçµæœç‚º" + user_result
```
```python
import requests
import subprocess

def CALL_API (user_question):
    
    API_URL = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1"
    headers = {"Authorization": "Bearer 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"}

    def query(payload):
        response = requests.post(API_URL, headers=headers, json=payload)
        return response.json()

    API_fetch = query({
        "inputs": user_question,
    })

    return API_fetch

API_fetch = CALL_API(user_question)
print(API_fetch)
```
```python
import re

def json_processed (API_fetch):
    text = API_fetch[0]['generated_text']
    code_blocks = re.findall(r'```\n(.*?)\n```', text, re.DOTALL)

    
    return block

json_processed = json_processed(API_fetch)
```
```python
situation = 1 #0æˆåŠŸã€1å¤±æ•—
error = 0
```
```python
while situation == 1:
    if error == 1:
        new_API_fetch = CALL_API(stderr)
        print(new_API_fetch)
        
    #è£½ä½œä¸€å€‹test.txt
    path = 'test.txt'
    f = open(path, 'w')
    f.write(str(json_processed))
    f.close()
    
    #åŸ·è¡Œtest.txt
    # è¨­å®šè¦åŸ·è¡Œçš„Pythonè…³æœ¬æ–‡ä»¶
    command = "python test.txt"

    # ä½¿ç”¨subprocessåŸ·è¡Œå‘½ä»¤
    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

    # ç­‰å¾…å‘½ä»¤å®Œæˆ
    stdout, stderr = process.communicate()
    stdout = stdout.strip()
    #print(stdout)

    
    # æª¢æŸ¥å‘½ä»¤æ˜¯å¦æˆåŠŸ
    if process.returncode == 0 and stdout == user_result:
        print("å‘½ä»¤æˆåŠŸåŸ·è¡Œï¼Œè¼¸å‡ºç‚ºï¼š")
        situation = 0
        print(stdout)
    else:
        print("å‘½ä»¤åŸ·è¡Œå¤±æ•—ï¼ŒéŒ¯èª¤è¨Šæ¯ç‚ºï¼š")
        situation = 1
        error = 1
        stderr = stderr + "This is not the result I want, the result I want is " + user_result + ". Can you generate a new code?"
        print(stderr)
```
## 11/6ï½œPre-Discussion
https://colab.research.google.com/drive/1ZveGLd_uMhTei-EAtOuOpsnPVgh7Bu3k?usp=sharing
### Fine-tuning
#### Upload a training file
```python
import os
import openai
openai.api_key = os.getenv("OPENAI_API_KEY")
openai.File.create(
  file=open("mydata.jsonl", "rb"),
  purpose='fine-tune'
)
```
### Create a fine-tuned model
```python
openai.FineTuningJob.create(training_file="file-abc123", model="gpt-3.5-turbo")
```
### Fine-tuning GPT-2
#### Test in huggingface
```python
#gpt-2
import requests

API_URL = "https://api-inference.huggingface.co/models/gpt2"
headers = {"Authorization": "Bearer {API_KEY}"}

def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({
	"inputs": "Can you generate a python code to make a bar chart based on matplotlib?",
})
```
#### Test in Colab
```python
pip install transformers
```
```python
pip install torch
```
```python
from google.colab import drive
drive.mount('/content/drive')
f = open('/content/drive/MyDrive/Colab Notebooks/test.txt','r')
a = f.read()
print(a)
f.close()
```
```python
import accelerate
from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# å‡†å¤‡æ•°æ®é›†
train_path = '/content/drive/MyDrive/Colab Notebooks/train.txt'
test_path = '/content/drive/MyDrive/Colab Notebooks/test.txt'

train_dataset = TextDataset(
  tokenizer=tokenizer,
  file_path=train_path,
  block_size=128
)

test_dataset = TextDataset(
  tokenizer=tokenizer,
  file_path=test_path,
  block_size=128
)

data_collator = DataCollatorForLanguageModeling(
  tokenizer=tokenizer, mlm=False
)

# å®šä¹‰è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./gpt2-finetuned",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    eval_steps=400,
    save_steps=800,
    warmup_steps=500,
    prediction_loss_only=True,
)

# åˆå§‹åŒ–è®­ç»ƒå™¨å¹¶å¼€å§‹å¾®è°ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

trainer.train()

```
## 11/7ï½œMeeting with Dr.Tsai
### word2vec
https://www.tensorflow.org/text/tutorials/word2vec
https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/word2vec.ipynb

### BERT
https://mofanpy.com/tutorials/machine-learning/nlp/cbow

### æ–¹å‘ç¢ºèª
æ‡‰è©²å…ˆembedding å†ä¾†è€ƒæ…®fine-tuning

## 11/8ï½œEmbedding
https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/videos/semantic_search.ipynb
### å°‡å¥å­ç·¨ç¢¼æˆåµŒå…¥å‘é‡
:::spoiler å°‡å¥å­ç·¨ç¢¼æˆåµŒå…¥å‘é‡

```
ä½¿ç”¨é è¨“ç·´çš„èªè¨€æ¨¡å‹==å°‡å¥å­ç·¨ç¢¼æˆåµŒå…¥å‘é‡==
```

é€™è¡Œä»£ç¢¼å°å…¥äº†PyTorchåº«ï¼Œé€™æ˜¯ä¸€å€‹åœ¨Pythonä¸­å¯¦ç¾æ·±åº¦å­¸ç¿’çš„æµè¡Œåº«ã€‚

```python
import torch
```
å¾transformersåº«ä¸­å°å…¥AutoTokenizerå’ŒAutoModelã€‚é€™å€‹åº«æä¾›äº†è¨±å¤šé è¨“ç·´çš„æ¨¡å‹ï¼Œç”¨æ–¼è‡ªç„¶èªè¨€è™•ç†ä»»å‹™ã€‚
```python
from transformers import AutoTokenizer, AutoModel
```
å®šç¾©äº†ä¸€å€‹åˆ—è¡¨sentencesï¼ŒåŒ…å«ä¸‰å€‹==è¦è™•ç†çš„è‹±æ–‡å¥å­==ã€‚
```python
sentences = [
    "I took my dog for a walk",
    "Today is going to rain",
    "I took my cat for a walk",
]
```
è¨­ç½®ä¸€å€‹è®Šé‡model_ckptï¼Œå®ƒåŒ…å«äº†==é è¨“ç·´æ¨¡å‹çš„åç¨±==ï¼Œç”¨æ–¼å¾ŒçºŒåŠ è¼‰æ¨¡å‹å’Œåˆ†è©å™¨ã€‚
```python
model_ckpt = "sentence-transformers/all-MiniLM-L6-v2"
```
ä½¿ç”¨model_ckptä¸­æŒ‡å®šçš„æ¨¡å‹åç¨±å‰µå»ºä¸€å€‹åˆ†è©å™¨å¯¦ä¾‹ï¼Œé€™å€‹åˆ†è©å™¨å°‡ç”¨æ–¼å°‡å¥å­è½‰æ›æˆ==æ¨¡å‹èƒ½ç†è§£çš„æ ¼å¼==ã€‚
```python
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
```
åŠ è¼‰èˆ‡åˆ†è©å™¨ç›¸å°æ‡‰çš„èªè¨€æ¨¡å‹ï¼Œé€™å€‹æ¨¡å‹å°‡ç”¨ä¾†==ç”ŸæˆåµŒå…¥å‘é‡==ã€‚
```python
model = AutoModel.from_pretrained(model_ckpt)
```
èª¿ç”¨åˆ†è©å™¨å°‡å¥å­åˆ—è¡¨è½‰æ›æˆæ¨¡å‹éœ€è¦çš„è¼¸å…¥æ ¼å¼ï¼Œä¸¦å°‡å®ƒå€‘==è½‰æ›æˆPyTorchå¼µé‡==ã€‚é€™è£¡é‚„è¨­ç½®äº†å¡«å……ï¼ˆä»¥ä½¿æ‰€æœ‰å¥å­é•·åº¦ä¸€è‡´ï¼‰å’Œæˆªæ–·ï¼ˆä»¥é¿å…è¶…é•·å¥å­ï¼‰ã€‚
```python
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors="pt")
```
åœ¨ä¸è¨ˆç®—æ¢¯åº¦çš„æƒ…æ³ä¸‹åŸ·è¡Œæ¨¡å‹ï¼ˆé€™åœ¨æ¨è«–æ™‚ç¯€çœè¨ˆç®—è³‡æºï¼‰ï¼Œä¸¦å°‡ç·¨ç¢¼å¾Œçš„è¼¸å…¥æä¾›çµ¦æ¨¡å‹ï¼Œ==å¾—åˆ°æ¨¡å‹çš„è¼¸å‡º==ã€‚
```python
with torch.no_grad():
    model_output = model(**encoded_input)
```
å¾æ¨¡å‹è¼¸å‡ºä¸­å–å‡ºæœ€å¾Œä¸€å±¤éš±è—ç‹€æ…‹ï¼Œé€™ä»£è¡¨äº†==è¼¸å…¥å¥å­ä¸­æ¯å€‹tokençš„åµŒå…¥å‘é‡==ã€‚
```python
token_embeddings = model_output.last_hidden_state
```
æ‰“å°å‡ºtokenåµŒå…¥çš„å½¢ç‹€ï¼Œé€™æœƒé¡¯ç¤ºå‡º==å¼µé‡çš„ç¶­åº¦==ï¼Œé€šå¸¸æ˜¯ï¼ˆ==å¥å­æ•¸é‡, æ¯å¥è©±çš„tokenæ•¸é‡, åµŒå…¥ç¶­åº¦==ï¼‰ã€‚
é€™å‘Šè¨´æˆ‘å€‘æ¨¡å‹ç”¢ç”Ÿäº†å¤šå°‘å€‹åµŒå…¥å‘é‡ï¼Œä»¥åŠæ¯å€‹å‘é‡çš„å¤§å°ã€‚
```python
print(f"Token embeddings shape: {token_embeddings.size()}")
```
Output: ```Token embeddings shape: torch.Size([3, 9, 384])```

:::
### ç”¢ç”Ÿå¥å­ç´šçš„åµŒå…¥ä¸¦æ¨™æº–åŒ–

:::spoiler ç”¢ç”Ÿå¥å­ç´šçš„åµŒå…¥ä¸¦æ¨™æº–åŒ–
```
å®šç¾©mean_poolingï¼šè™•ç†ç”±transformeræ¨¡å‹ï¼ˆå¦‚BERTæˆ–ç›¸ä¼¼æ¨¡å‹ï¼‰ç”Ÿæˆçš„tokenåµŒå…¥ï¼Œä¸¦ä¸”ç”¢ç”Ÿå¥å­ç´šçš„åµŒå…¥ã€‚

æ¥è‘—ï¼Œå®ƒæ¨™æº–åŒ–é€™äº›åµŒå…¥ä¸¦æ‰“å°å‡ºåµŒå…¥çš„å½¢ç‹€ã€‚
```

é€™è¡Œä»£ç¢¼==å°å…¥PyTorchæ·±åº¦å­¸ç¿’åº«ä¸­çš„functionalæ¨¡çµ„==ï¼Œé€šå¸¸ç°¡ç¨±ç‚ºFã€‚é€™å€‹æ¨¡çµ„åŒ…å«äº†ä¸€çµ„å‡½æ•¸ï¼Œé€™äº›å‡½æ•¸å¯ä»¥åœ¨==ä¸éœ€è¦å®šç¾©å®Œæ•´ç¥ç¶“ç¶²çµ¡å±¤çš„æƒ…æ³ä¸‹ï¼Œç›´æ¥å°æ•¸æ“šé€²è¡Œæ“ä½œ==ã€‚
```python
import torch.nn.functional as F
```
å®šç¾©ä¸€å€‹åç‚ºmean_poolingçš„å‡½æ•¸
```python
# å®ƒæ¥å—æ¨¡å‹è¼¸å‡ºå’Œæ³¨æ„åŠ›é®ç½©ï¼ˆattention maskï¼‰ä½œç‚ºåƒæ•¸ã€‚
def mean_pooling(model_output, attention_mask):
    
    #å¾æ¨¡å‹è¼¸å‡ºä¸­æå–æœ€å¾Œä¸€å±¤éš±è—ç‹€æ…‹ï¼Œé€™åŒ…å«äº†å¥å­ä¸­æ¯å€‹tokençš„åµŒå…¥è¡¨ç¤ºã€‚
    token_embeddings = model_output.last_hidden_state
    
    #é€™è¡Œä»£ç¢¼å°‡æ³¨æ„åŠ›é®ç½©å±•é–‹åˆ°èˆ‡tokenåµŒå…¥ç›¸åŒçš„å°ºå¯¸ã€‚unsqueeze(-1)åœ¨æœ€å¾Œä¸€å€‹ç¶­åº¦ä¸Šå¢åŠ ä¸€å€‹è»¸ï¼Œä½¿å¾—é®ç½©å¯ä»¥é€šéexpandæ–¹æ³•å»¶å±•åˆ°èˆ‡åµŒå…¥ç›¸åŒçš„å½¢ç‹€ã€‚
    input_mask_expanded = (
        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    )
    
    # å°‡æ“´å±•å¾Œçš„é®ç½©æ‡‰ç”¨æ–¼tokenåµŒå…¥ï¼Œç„¶å¾Œå°æ¯å€‹å¥å­é€²è¡Œæ±‚å’Œï¼Œå¾—åˆ°åŠ æ¬Šçš„tokenåµŒå…¥ç¸½å’Œã€‚é€™å€‹ç¸½å’Œé€šéæ¯å€‹å¥å­çš„éé›¶tokenæ•¸é‡é€²è¡Œæ­¸ä¸€åŒ–ï¼Œä½¿ç”¨torch.clampä¾†é¿å…é™¤ä»¥é›¶çš„æƒ…æ³ï¼Œæœ€å°å€¼è¨­å®šç‚º1e-9ã€‚
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(
        input_mask_expanded.sum(1), min=1e-9
    )

```
é€™è¡Œä»£ç¢¼èª¿ç”¨mean_poolingå‡½æ•¸ï¼Œ==å°‡æ¨¡å‹è¼¸å‡ºå’Œç·¨ç¢¼è¼¸å…¥ä¸­çš„æ³¨æ„åŠ›é®ç½©ä½œç‚ºåƒæ•¸å‚³å…¥==ï¼Œå¾—åˆ°==æ¯å€‹å¥å­çš„åµŒå…¥==ã€‚
```python
sentence_embeddings = mean_pooling(model_output, encoded_input["attention_mask"])
```
ä½¿ç”¨PyTorchçš„functional APIä¸­çš„normalizeå‡½æ•¸å°å¥å­åµŒå…¥é€²è¡Œ==L2æ­¸ä¸€åŒ–==ï¼Œé€™æœ‰åŠ©æ–¼åœ¨å¾ŒçºŒçš„ä»»å‹™ä¸­æ”¹å–„æ¨¡å‹çš„æ€§èƒ½å’Œç©©å®šæ€§ã€‚
```python
# Normalize the embeddings
sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)
```
æœ€å¾Œï¼Œé€™è¡Œä»£ç¢¼æ‰“å°å‡º==ç¶“éæ­¸ä¸€åŒ–è™•ç†çš„å¥å­åµŒå…¥å‘é‡çš„å½¢ç‹€==ã€‚é€™æœƒé¡¯ç¤ºå¼µé‡çš„ç¶­åº¦ï¼Œé€šå¸¸æ˜¯ï¼ˆ==å¥å­æ•¸é‡, åµŒå…¥å‘é‡ç¶­åº¦==ï¼‰ã€‚
é€™å‘Šè¨´æˆ‘å€‘ç¶“éè™•ç†å¾Œå¾—åˆ°äº†==å¤šå°‘å€‹å¥å­åµŒå…¥ã€ä»¥åŠæ¯å€‹åµŒå…¥çš„å¤§å°==ã€‚
```python
print(f"Sentence embeddings shape: {sentence_embeddings.size()}")
```
Output: ```Sentence embeddings shape: torch.Size([3, 384])```
:::

### è¨ˆç®—ä¸€çµ„å¥å­åµŒå…¥ä¹‹é–“çš„é¤˜å¼¦ç›¸ä¼¼åº¦
:::spoiler è¨ˆç®—ä¸€çµ„å¥å­åµŒå…¥ä¹‹é–“çš„é¤˜å¼¦ç›¸ä¼¼åº¦
é€™è¡Œä»£ç¢¼å°å…¥äº†==NumPy==åº«ï¼Œé€™æ˜¯Pythonä¸­ç”¨æ–¼é€²è¡Œç§‘å­¸è¨ˆç®—çš„åŸºç¤åº«ï¼Œæä¾›äº†å¤§é‡çš„æ•¸å­¸å‡½æ•¸å’Œå¤šç¶­é™£åˆ—æ“ä½œã€‚
```python
import numpy as np
```
å¾sklearn.metrics.pairwiseæ¨¡çµ„ä¸­å°å…¥cosine_similarityå‡½æ•¸ã€‚sklearnï¼ˆScikit-learnï¼‰æ˜¯ä¸€å€‹æä¾›è¨±å¤šå¸¸è¦‹==æ©Ÿå™¨å­¸ç¿’ç®—æ³•çš„Pythonåº«==ï¼Œcosine_similarityç”¨æ–¼è¨ˆç®—å‘é‡ä¹‹é–“çš„==é¤˜å¼¦ç›¸ä¼¼åº¦==ã€‚
```python
from sklearn.metrics.pairwise import cosine_similarity
```
é€™è¡Œä»£ç¢¼å°‡PyTorchå¼µé‡ä¸­çš„==å¥å­åµŒå…¥è½‰æ›æˆNumPyé™£åˆ—==ã€‚detach()æ–¹æ³•å°‡åµŒå…¥å¾ç•¶å‰è¨ˆç®—åœ–ä¸­åˆ†é›¢å‡ºä¾†ï¼Œé€™æ¨£å¯ä»¥é˜²æ­¢åœ¨è½‰æ›æˆNumPyé™£åˆ—æ™‚ç™¼ç”Ÿæ¢¯åº¦ä¿¡æ¯çš„éŒ¯èª¤å‚³éã€‚
```python
sentence_embeddings = sentence_embeddings.detach().numpy()
```
åˆå§‹åŒ–ä¸€å€‹äºŒç¶­é™£åˆ—scoresï¼Œç”¨é›¶å¡«å……ï¼Œå…¶å½¢ç‹€ç”±å¥å­åµŒå…¥çš„æ•¸é‡æ±ºå®šã€‚
é€™å€‹é™£åˆ—å°‡ç”¨ä¾†==å„²å­˜æ‰€æœ‰å¥å­åµŒå…¥ä¹‹é–“çš„é¤˜å¼¦ç›¸ä¼¼åº¦åˆ†æ•¸==ã€‚
```python
scores = np.zeros((sentence_embeddings.shape[0], sentence_embeddings.shape[0]))
```
é–‹å§‹ä¸€å€‹å¾ªç’°ï¼Œ==å°æ¯å€‹å¥å­åµŒå…¥é€²è¡Œè¿­ä»£==ã€‚
```python!
for idx in range(sentence_embeddings.shape[0]):
    
    # å°æ–¼æ¯å€‹å¥å­åµŒå…¥ï¼Œè¨ˆç®—å®ƒèˆ‡æ‰€æœ‰å…¶ä»–å¥å­åµŒå…¥çš„é¤˜å¼¦ç›¸ä¼¼åº¦ã€‚
    # cosine_similarityå‡½æ•¸æ¥å—å…©å€‹åƒæ•¸ï¼šä¸€å€‹æ˜¯å–®å€‹å¥å­åµŒå…¥ï¼ˆéœ€è¦åŒ…è£åœ¨åˆ—è¡¨ä¸­ï¼Œå› ç‚ºcosine_similarityæœŸæœ›äºŒç¶­é™£åˆ—ï¼‰ï¼Œå¦ä¸€å€‹æ˜¯æ•´å€‹å¥å­åµŒå…¥é™£åˆ—ã€‚
    # å‡½æ•¸è¿”å›ä¸€å€‹ä¸€ç¶­é™£åˆ—ï¼ŒåŒ…å«äº†ç•¶å‰å¥å­åµŒå…¥èˆ‡é™£åˆ—ä¸­æ¯å€‹å¥å­åµŒå…¥çš„ç›¸ä¼¼åº¦åˆ†æ•¸ï¼Œé€™äº›åˆ†æ•¸è¢«è³¦å€¼çµ¦scoresé™£åˆ—ä¸­å°æ‡‰çš„è¡Œã€‚
    
    scores[idx, :] = cosine_similarity([sentence_embeddings[idx]], sentence_embeddings)[0]
```
:::
### ç²å–æ–‡æœ¬çš„åµŒå…¥å‘é‡
::: spoiler ç²å–æ–‡æœ¬çš„åµŒå…¥å‘é‡
```
é€™æ®µç¨‹å¼ç¢¼ä½¿ç”¨äº†datasetsåº«ä¾†åŠ è¼‰å’Œé è™•ç†SQuADï¼ˆStanford Question Answering Datasetï¼‰æ•¸æ“šé›†ï¼Œä¸¦ä¸”å®šç¾©äº†ä¸€å€‹å‡½æ•¸ä¾†ç²å–æ–‡æœ¬çš„åµŒå…¥å‘é‡ã€‚
```
é€™è¡Œä»£ç¢¼å°å…¥äº†datasetsåº«çš„load_datasetå‡½æ•¸ï¼Œé€™å€‹å‡½æ•¸ç”¨æ–¼åŠ è¼‰å’Œè™•ç†å…¬é–‹å¯ç”¨çš„æ•¸æ“šé›†ã€‚
```python
from datasets import load_dataset
```
åŠ è¼‰SQuADæ•¸æ“šé›†çš„é©—è­‰é›†éƒ¨åˆ†ï¼Œä½¿ç”¨ç¨®å­42é€²è¡Œéš¨æ©Ÿæ‰“äº‚ï¼Œç„¶å¾Œé¸æ“‡å‰100å€‹æ¨£æœ¬ã€‚shuffleæ˜¯ç‚ºäº†ç¢ºä¿é¸æ“‡çš„æ˜¯éš¨æ©Ÿçš„æ¨£æœ¬ï¼Œè€Œselectå‰‡æ˜¯å¾æ‰“äº‚å¾Œçš„æ•¸æ“šé›†ä¸­é¸æ“‡ä¸€å€‹å­é›†ã€‚
```python
squad = load_dataset("squad", split="validation").shuffle(seed=42).select(range(100))
```
å®šç¾©äº†ä¸€å€‹å‡½æ•¸get_embeddingsï¼Œå®ƒæ¥å—ä¸€å€‹æ–‡æœ¬åˆ—è¡¨ä½œç‚ºåƒæ•¸ã€‚
```python
def get_embeddings(text_list):
    
    #ä½¿ç”¨å…ˆå‰æåˆ°çš„åˆ†è©å™¨å°‡æ–‡æœ¬åˆ—è¡¨è½‰æ›ç‚ºæ¨¡å‹éœ€è¦çš„æ ¼å¼ï¼Œé€²è¡Œå¡«å……å’Œæˆªæ–·ä»¥ä¿æŒä¸€è‡´é•·åº¦ï¼Œä¸¦å°‡è½‰æ›å¾Œçš„è¼¸å…¥è½‰æ›ç‚ºPyTorchå¼µé‡ã€‚
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    
    #é€™è¡Œä»£ç¢¼å°‡åˆ†è©å™¨è¼¸å‡ºçš„å­—å…¸é€²è¡Œäº†è§£åŒ…ï¼Œå®ƒå¯¦éš›ä¸Šæ²’æœ‰æ”¹è®Šä»»ä½•å…§å®¹ï¼Œå¯èƒ½æ˜¯ç‚ºäº†æ¸…æ™°è¡¨ç¤ºè¼¸å…¥çš„çµæ§‹æˆ–æ˜¯ä¿®å¾©æŸäº›ç’°å¢ƒä¸‹çš„ç›¸å®¹æ€§å•é¡Œã€‚
    encoded_input = {k: v for k, v in encoded_input.items()}
    
    #åœ¨ç„¡éœ€è¨ˆç®—æ¢¯åº¦çš„ä¸Šä¸‹æ–‡ä¸­åŸ·è¡Œæ¨¡å‹ï¼Œé€™æ˜¯æ¨è«–éšæ®µå¸¸è¦‹çš„åšæ³•ï¼Œå¯ä»¥ç¯€çœè¨˜æ†¶é«”å’Œè¨ˆç®—æ™‚é–“ã€‚
    with torch.no_grad():
        model_output = model(**encoded_input)
        
    #è¿”å›ä½¿ç”¨mean_poolingå‡½æ•¸è™•ç†çš„æ¨¡å‹è¼¸å‡ºï¼Œè©²å‡½æ•¸ç”¢ç”Ÿå¥å­ç´šçš„åµŒå…¥ã€‚
    return mean_pooling(model_output, encoded_input["attention_mask"])

```
ä½¿ç”¨mapå‡½æ•¸éæ­·SQuADæ•¸æ“šé›†çš„æ¯ä¸€å€‹æ¨£æœ¬ï¼Œå°æ–¼æ¯å€‹æ¨£æœ¬çš„contextå­—æ®µï¼Œä½¿ç”¨get_embeddingså‡½æ•¸è¨ˆç®—åµŒå…¥å‘é‡ã€‚è¨ˆç®—å¾Œï¼Œå°‡åµŒå…¥å¾PyTorchçš„CUDAå¼µé‡è½‰æ›åˆ°CPUå¼µé‡ï¼Œå†å°‡å…¶è½‰ç‚ºNumPyé™£åˆ—ï¼Œä¸¦å°‡çµæœå­˜å„²åœ¨æ–°çš„å­—æ®µembeddingsä¸­ã€‚é€™å€‹éç¨‹å°‡ç‚ºSQuADæ•¸æ“šé›†çš„æ¯å€‹æ¨£æœ¬æ·»åŠ å°æ‡‰çš„åµŒå…¥å‘é‡ã€‚

```python
squad_with_embeddings = squad.map(
    lambda x: {"embeddings": get_embeddings(x["context"]).cpu().numpy()[0]}
)
```
:::
### Last
:::spoiler Last
```
é€™æ®µç¨‹å¼ç¢¼å°åŒ…å«åµŒå…¥å‘é‡çš„SQuADæ•¸æ“šé›†å»ºç«‹äº†ä¸€å€‹FAISSç´¢å¼•ï¼Œä»¥ä¾¿å¿«é€Ÿé€²è¡Œé«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢ï¼Œç„¶å¾Œå°ä¸€å€‹ç‰¹å®šçš„å•é¡Œé€²è¡Œç·¨ç¢¼ä¸¦æª¢ç´¢æœ€ç›¸é—œçš„æ•¸æ“šé›†æ¢ç›®ã€‚é€™æ®µç¨‹å¼ç¢¼å°åŒ…å«åµŒå…¥å‘é‡çš„SQuADæ•¸æ“šé›†å»ºç«‹äº†ä¸€å€‹FAISSç´¢å¼•ï¼Œä»¥ä¾¿å¿«é€Ÿé€²è¡Œé«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢ï¼Œç„¶å¾Œå°ä¸€å€‹ç‰¹å®šçš„å•é¡Œé€²è¡Œç·¨ç¢¼ä¸¦æª¢ç´¢æœ€ç›¸é—œçš„æ•¸æ“šé›†æ¢ç›®ã€‚
```
:::

### w3school python çˆ¬èŸ²
æš«æ™‚åªçˆ¬å‡ºå…¶ä¸­ä¸€é (https://www.w3schools.com/python/python_getstarted.asp)

```python
from bs4 import BeautifulSoup 
import requests
import re
import pandas as pd

url="https://www.w3schools.com/python/python_getstarted.asp"
headers={
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
}

res = requests.get(url, headers=headers)
soup = BeautifulSoup(res.text, "html.parser")
sample_t=[]
# æŠ“å–å…§å®¹
sample = soup.find_all("div", class_="w3-example")
container_div = soup.find("div", class_="w3-col l10 m12")
if container_div:
    p_elements = container_div.find_all(['p', 'li'])
    p_elements_t = "\n".join([content.get_text() for content in p_elements])
for div in sample:
    text = div.text
    index = text.find(">>>")
    if index != -1:
        text = text[index + 3:]

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»é™¤æ¢è¡Œå’Œç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    sample_t.append(text)
print(p_elements_t)
print("-"*50)
sample_t
```

### embedding code
[colab URL](https://colab.research.google.com/drive/1tKJ6cyL9y6Y8Fbtatcp1PhTcA8RDPdYO?usp=sharing)*ä½¿ç”¨å¸«å¤§å¸³è™Ÿã€å»ºç«‹å‰¯æœ¬*

[Three little pigs](https://github.com/gbishop/samisays/blob/master/samisays/three%20little%20pigs.txt)
[Harry Potter](https://github.com/bobdeng/owlreader/blob/master/ERead/assets/books/Harry%20Potter%20and%20The%20Half-Blood%20Prince.txt)

### embedding adjust
åœ¨ Word2Vec æ¨¡å‹ä¸­ï¼Œå„å€‹å¯èª¿æ•´è®Šé‡çš„ç¯„åœå’ŒåŠŸèƒ½å¦‚ä¸‹ï¼š

- `vector_size`ï¼š==è©å‘é‡çš„ç¶­åº¦==ï¼Œå¯ä»¥æ˜¯ä»»ä½•æ­£æ•´æ•¸ã€‚
    - å¢åŠ ç¶­åº¦å¯ä»¥æé«˜æ¨¡å‹çš„è¤‡é›œåº¦å’Œå®¹é‡ï¼Œä½†ä¹Ÿå¯èƒ½å°è‡´éæ“¬åˆï¼Œç‰¹åˆ¥æ˜¯åœ¨è©å½™é‡ä¸å¤§çš„æ•¸æ“šé›†ä¸Šã€‚é€šå¸¸ç¯„åœåœ¨ 50 åˆ° 300 ä¹‹é–“ã€‚

- `window`ï¼š==ä¸Šä¸‹æ–‡çª—å£å¤§å°==ï¼Œä¹Ÿæ˜¯æ­£æ•´æ•¸ã€‚çª—å£å¤§å°æ±ºå®šäº†åœ¨è€ƒæ…®ç›®æ¨™è©çš„ä¸Šä¸‹æ–‡æ™‚è¦è€ƒæ…®çš„å‘¨åœè©èªçš„ç¯„åœã€‚
    - çª—å£å¤§å°è¼ƒå°å¯èƒ½æœƒè®“æ¨¡å‹å­¸ç¿’åˆ°æ›´å¤šé—œæ–¼è©èªç‰¹å®šç”¨é€”çš„ä¿¡æ¯ï¼Œè€Œè¼ƒå¤§çš„çª—å£å‰‡æœ‰åŠ©æ–¼å­¸ç¿’è©èªçš„å»£æ³›ç”¨é€”ã€‚ä¸€èˆ¬ä¾†èªªï¼Œçª—å£å¤§å°è¨­ç½®åœ¨ 5 åˆ° 20 ä¹‹é–“ã€‚

- `min_count`ï¼š==è©é »ä¸‹é™==ï¼Œæ±ºå®šäº†è©å¿…é ˆå‡ºç¾çš„æœ€å°æ¬¡æ•¸æ‰èƒ½è¢«è€ƒæ…®é€²è©å½™è¡¨ã€‚é€™æ˜¯ä¸€å€‹éè² æ•´æ•¸ã€‚
    - å°‡æ­¤å€¼è¨­ç½®å¾—å¤ªä½å¯èƒ½æœƒå°è‡´è¨±å¤šç½•è¦‹è©æ±¡æŸ“è©å‘é‡ç©ºé–“ï¼Œè¨­ç½®å¾—å¤ªé«˜å¯èƒ½æœƒå¿½ç•¥æœ‰ç”¨çš„ä¿¡æ¯ã€‚é€šå¸¸è¨­ç½®ç‚º 1 åˆ° 100ã€‚

- `workers`ï¼š==é€²è¡Œè¨“ç·´çš„ç·šç¨‹æ•¸é‡==ï¼Œé€™å€‹æ•¸å­—æ‡‰è©²å’Œä½ çš„æ©Ÿå™¨çš„è™•ç†å™¨æ ¸å¿ƒæ•¸ç›¸ç¬¦ã€‚é€™æ˜¯ä¸€å€‹æ­£æ•´æ•¸ï¼Œå¦‚æœä½ çš„æ©Ÿå™¨æœ‰å¤šå€‹æ ¸å¿ƒï¼Œå¢åŠ é€™å€‹å€¼å¯ä»¥åŠ å¿«è¨“ç·´é€Ÿåº¦ã€‚
    - ç„¶è€Œï¼Œé€™ä¸¦ä¸æœƒå½±éŸ¿è¨“ç·´å¾Œæ¨¡å‹çš„æ€§èƒ½æˆ–è³ªé‡ã€‚

é€™äº›è®Šé‡å¯ä»¥æ ¹æ“šå…·é«”çš„æ•¸æ“šé›†å’Œæ‡‰ç”¨å ´æ™¯é€²è¡Œèª¿æ•´ï¼Œä»¥ç²å¾—æœ€ä½³çš„æ¨¡å‹è¡¨ç¾ã€‚ä¾‹å¦‚ï¼Œè¼ƒå°çš„æ•¸æ“šé›†å¯èƒ½éœ€è¦è¼ƒå°çš„ `vector_size` å’Œè¼ƒé«˜çš„ `min_count`ï¼Œè€Œè¼ƒå¤§çš„æ•¸æ“šé›†å‰‡å¯èƒ½éœ€è¦è¼ƒå¤§çš„ `vector_size` å’Œè¼ƒå°çš„ `min_count`ã€‚èª¿æ•´é€™äº›åƒæ•¸æ™‚ï¼Œå¯èƒ½éœ€è¦é€šéäº¤å‰é©—è­‰ç­‰æ–¹æ³•ä¾†æ‰¾åˆ°æœ€å„ªçš„åƒæ•¸çµ„åˆã€‚

## 11/13ï½œPre-Discussion
https://hackmd.io/@meebox/SJ7Um4Whs

### Selenium Web Crawler for embedding data
:::spoiler Code
https://github.com/knyliu/LATIA112-1/blob/main/LATIA_HW2
:::
### Scrapy for embedding data
:::spoiler Code
https://github.com/knyliu/LATIA112-1/blob/main/LATIA_HW2

Create a project in terminal:
```
scrapy startproject w3schools_scrapy
```
Start a project in terminal:
```
scrapy crawl w3schools
```
"w3schools" is depended upon for the name in the Python code.
:::
---
> [name=åŠ‰å½¥è°·]
> @mason45ok å¯«çš„ä¸€é çˆ¬èŸ²ï¼Œä¸ä½†åªèƒ½çˆ¬ä¸€é ã€å–®ä¸€é åˆçˆ¬ä¸å®Œæ•´ã€‚

> [name=mason45ok]
> æˆ‘å¾ˆæŠ±æ­‰ğŸ˜¢

:::warning
To Be Discussed:
1. æœ‰äº›è³‡æ–™ï¼ˆé é¢ï¼‰åœ¨çˆ¬èŸ²æ™‚æœƒè¢«è·³é
    ä¾‹å¦‚ï¼špython try...except
2. æœ‰äº›é é¢åº•ä¸‹é‚„æœ‰æ›´å¤šé é¢
    ä¾‹å¦‚ï¼šPython Stringã€‚
3. æœ‰äº›é é¢çš„å…§å®¹æœƒé‡è¤‡å­˜åˆ°csv fileä¸­
4. å¾Œé¢çš„é æ•¸å³ä¾¿XPathæ˜¯æ­£ç¢ºçš„ï¼ˆåœ¨ç¶²é ä¸­å­˜åœ¨æ­¤Xpathï¼‰ï¼Œå»ä¹Ÿç„¡æ³•çˆ¬èŸ²ã€‚è¿´åœˆæœ‰å¯«æª¢æ¸¬æ©Ÿåˆ¶ï¼Œæœƒå ±éŒ¯ï¼šElement not found for Xpathã€‚
    ä¸ç¢ºå®šæ˜¯ä¸æ˜¯è¢«æ“‹ï¼Ÿä½†åˆæ„Ÿè¦ºä¸æ˜¯ã€‚
:::
### BERT
https://mofanpy.com/tutorials/machine-learning/nlp/bert
> [name=åŠ‰å½¥è°·]
> We can do it after Mason finishes the web crawler.

## 11/15ï½œBERT Model Trainning
è¦å¾é ­é–‹å§‹è¨“ç·´ä¸€å€‹ç©ºçš„BERTæ¨¡å‹ï¼Œæ‚¨éœ€è¦éµå¾ªä»¥ä¸‹æ­¥é©Ÿï¼š

1. **æ•¸æ“šæº–å‚™**ï¼šBERTçš„è¨“ç·´éœ€è¦å¤§é‡çš„æ–‡æœ¬æ•¸æ“šã€‚é€™äº›æ•¸æ“šé€šå¸¸æ˜¯æœªæ¨™è¨˜çš„ï¼Œä¾†è‡ªå„ç¨®ä¾†æºï¼Œä¾‹å¦‚æ›¸ç±ã€ç¶²ç«™å’Œæ–°èæ–‡ç« ã€‚æ‚¨éœ€è¦æ”¶é›†ä¸¦é è™•ç†é€™äº›æ•¸æ“šï¼ŒåŒ…æ‹¬åˆ†è©ï¼ˆTokenizationï¼‰ã€å»é™¤ç‰¹æ®Šå­—ç¬¦ã€çµ±ä¸€å¤§å°å¯«ç­‰ã€‚

2. **é¸æ“‡é è¨“ç·´ä»»å‹™**ï¼šBERTé€šå¸¸é€šéå…©ç¨®é è¨“ç·´ä»»å‹™ä¾†è¨“ç·´ï¼šæ©ç¢¼èªè¨€æ¨¡å‹ï¼ˆMasked Language Model, MLMï¼‰å’Œä¸‹ä¸€å¥é æ¸¬ï¼ˆNext Sentence Prediction, NSPï¼‰ã€‚åœ¨MLMä¸­ï¼Œéš¨æ©Ÿåœ°å¾è¼¸å…¥å¥å­ä¸­é®è”½ä¸€äº›å–®è©ï¼Œç„¶å¾Œè®“æ¨¡å‹é æ¸¬é€™äº›å–®è©ã€‚åœ¨NSPä¸­ï¼Œæ¨¡å‹éœ€è¦é æ¸¬å…©å€‹å¥å­æ˜¯å¦é€£çºŒã€‚

3. **å»ºç«‹æ¨¡å‹æ¶æ§‹**ï¼šBERTæ˜¯ä¸€å€‹åŸºæ–¼Transformerçš„æ¨¡å‹ï¼Œå…·æœ‰å¤šå±¤è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰å±¤ã€‚æ‚¨éœ€è¦æ ¹æ“šéœ€è¦é¸æ“‡åˆé©çš„å±¤æ•¸ã€éš±è—å–®å…ƒæ•¸ã€é ­çš„æ•¸ç›®ç­‰åƒæ•¸ã€‚

4. **è¨­ç½®è¶…åƒæ•¸**ï¼šæ‚¨éœ€è¦é¸æ“‡é©åˆæ‚¨æ•¸æ“šå’Œç¡¬ä»¶çš„å­¸ç¿’ç‡ã€æ‰¹å¤§å°ï¼ˆBatch Sizeï¼‰ã€è¨“ç·´å‘¨æœŸï¼ˆEpochsï¼‰ç­‰ã€‚

5. **è¨“ç·´æ¨¡å‹**ï¼šä½¿ç”¨æ‚¨çš„æ•¸æ“šå’Œè¨­ç½®ï¼Œé–‹å§‹è¨“ç·´éç¨‹ã€‚é€™é€šå¸¸éœ€è¦ä½¿ç”¨GPUæˆ–TPUç­‰é«˜æ€§èƒ½è¨ˆç®—è³‡æºã€‚

6. **è©•ä¼°å’Œèª¿æ•´**ï¼šåœ¨è¨“ç·´éç¨‹ä¸­å’Œä¹‹å¾Œï¼Œè©•ä¼°æ¨¡å‹æ€§èƒ½ï¼Œä¸¦æ ¹æ“šéœ€è¦é€²è¡Œèª¿æ•´ã€‚é€™å¯èƒ½åŒ…æ‹¬èª¿æ•´è¶…åƒæ•¸ã€æ·»åŠ æ›´å¤šæ•¸æ“šæˆ–æ›´æ”¹é è™•ç†æ­¥é©Ÿã€‚

7. **å¾®èª¿**ï¼šä¸€æ—¦æ¨¡å‹åœ¨é è¨“ç·´ä»»å‹™ä¸Šè¡¨ç¾è‰¯å¥½ï¼Œæ‚¨å¯ä»¥é€šéåœ¨ç‰¹å®šä»»å‹™ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æã€å•ç­”ç­‰ï¼‰ä¸Šçš„é€²ä¸€æ­¥è¨“ç·´ä¾†å¾®èª¿å®ƒã€‚

è«‹æ³¨æ„ï¼Œè¨“ç·´BERTæ˜¯ä¸€å€‹è³‡æºå¯†é›†å’Œæ™‚é–“å¯†é›†çš„éç¨‹ï¼Œéœ€è¦å¤§é‡çš„è¨ˆç®—è³‡æºå’Œæ™‚é–“ã€‚å¦‚æœæ‚¨æ²’æœ‰è¶³å¤ çš„è³‡æºï¼Œå¯ä»¥è€ƒæ…®ä½¿ç”¨ç¾æˆçš„é è¨“ç·´æ¨¡å‹é€²è¡Œå¾®èª¿ï¼Œè€Œä¸æ˜¯å¾é ­é–‹å§‹è¨“ç·´ã€‚

:::spoiler BERT Code
```python
from transformers import BertTokenizer, BertForMaskedLM, BertConfig
from transformers import DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments
import torch
from torch.utils.data import Dataset

class TextDataset(Dataset):
    def __init__(self, tokenizer, file_path, block_size=512):
        self.tokenizer = tokenizer
        self.block_size = block_size
        self.examples = []

        with open(file_path, encoding="utf-8") as f:
            for line in f:
                batch_encoding = tokenizer(line, add_special_tokens=True, truncation=True, max_length=self.block_size, return_token_type_ids=False)
                self.examples.append(batch_encoding.input_ids)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        return torch.tensor(self.examples[i])

# åˆå§‹åŒ–tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")  # å¦‚æœæ‚¨çš„æ•°æ®æ˜¯ä¸­æ–‡

# åŠ è½½æ•°æ®é›†
file_path = 'harry_potter_1.txt'  # æ‚¨çš„æ•°æ®æ–‡ä»¶è·¯å¾„
dataset = TextDataset(tokenizer, file_path)

# åˆ›å»ºæ•°æ®collatorï¼Œç”¨äºåŠ¨æ€padding
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

# é…ç½®æ¨¡å‹
config = BertConfig.from_pretrained("bert-base-uncased")  # æ ¹æ®æ‚¨çš„éœ€æ±‚é€‰æ‹©é€‚å½“çš„é¢„è®­ç»ƒæ¨¡å‹
model = BertForMaskedLM(config)

# è®­ç»ƒå‚æ•°è®¾ç½®
training_args = TrainingArguments(
    output_dir="./bert_output",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
)

# åˆå§‹åŒ–Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

# å¼€å§‹è®­ç»ƒ
trainer.train()

```
:::

## 11/20ï½œPre-Discussion


|   | 11/29 | 12/6 |
| -------- | -------- | -------- |
| ä¸»è¦é€²åº¦     | Test the model     | PPT slides     |
| Note |      |Group Name|
| Daisy     | Text     | Text     |
| Kenny     | Text     | Text     |
| Mason     | Text     | Text     |
| Jessica     | Text     | Text     |

**11/22**
* Disscussion
    * ç’°å¢ƒset up 
    * code for trainning the model
    * train harry potter
* Home
    * clean the python data *1 (Sat. 11/25)
    * train the model for python *2
        * Kenny CANNOT train the model because of his terrible MacBook Pro.
    * Study how to test the model *1
