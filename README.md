# LLM LAB
**Members**
@Hsing-Tzu @kennyliou @imjessica @mason45ok


**11/2çªç„¶æƒ³åˆ°**
> [name=åŠ‰å½¥è°·]
å¦‚æžœpythonçš„è¼¸å‡ºæ˜¯ä½¿ç”¨è€…ç„¡æ³•é æ¸¬ï¼Œä¾‹å¦‚çˆ¬èŸ²å›žè¦†çš„çµæžœæˆ‘å€‘ç„¡æ³•é æ¸¬PTTçš„æ‰€æœ‰æ¨™é¡Œï¼Œé‚£æˆ‘å€‘å°±åœ¨å¾—åˆ°ç¬¬ä¸€æ¬¡API_fetchæ™‚ï¼Œå°å‡ºåŸ·è¡Œçµæžœè®“ä½¿ç”¨è€…å›žè¦†æ˜¯æˆ–å¦ï¼Œä»¥åˆ¤æ–·error = 1 or error = 0ã€‚

**11/2çªç„¶æƒ³åˆ° Part 2**
> [name=åŠ‰å½¥è°·]
æˆ‘å€‘ç¨‹å¼çš„éŽç¨‹æ˜¯ä¸æ˜¯å°±æ˜¯ä¸€å€‹fine-tuneçš„éŽç¨‹ï¼Œé‚£æˆ‘å€‘æ˜¯ä¸æ˜¯å¯ä»¥åŒæ™‚æŠŠjsonç”Ÿå‡ºä¾†

## 11/1ï½œFirst Discussion
### Topic 
è§£æ±ºè‡ªå·±åœ¨å­¸ç¿’Pythonæ™‚ï¼Œç•¶æˆ‘å€‘è©¢å•ChatGPTç›¸é—œå•é¡Œæ™‚ï¼Œæœƒå¾—åˆ°ä¸æº–ç¢ºçš„ç­”æ¡ˆï¼Œæˆ‘å€‘å¸Œæœ›èƒ½ä½¿å…¶ç­”å‡ºæº–ç¢ºçš„ç­”æ¡ˆï¼Œå› æ­¤ï¼Œæˆ‘å€‘å¸Œæœ›ChatGPTç”¢å‡ºç¨‹å¼ç¢¼ä¹‹å¾Œï¼Œå¯ä»¥å…ˆç·¨è­¯éŽï¼Œç¢ºèªéŽåŸ·è¡Œçµæžœæ˜¯æ­£ç¢ºçš„ï¼Œå†å›žå‚³ç¨‹å¼ç¢¼ã€‚
### Function
* API fetch
* jsonè™•ç†
    * å°‹æ‰¾å…©å°ä¸‰å€‹ã€Œ ` ã€æ‰€åŒ…å«çš„ç¨‹å¼ç¢¼ï¼Œä¸¦ä¸”åˆªé™¤ä¸å¿…è¦çš„ç¬¦è™Ÿ
* ç·¨è­¯
    * eval
* æª¢æŸ¥
    * eval çµæžœæ˜¯å¦ç­‰æ–¼æ¸¬è³‡ï¼Œæ˜¯ï¼šå›žå‚³pythonã€å¦ï¼šé‡è¤‡åŸ·è¡Œ
### ç¨‹å¼åŸ·è¡Œæµç¨‹
#### é€™æ˜¯ä¸€å€‹è˜‡éƒ½æ‰£
situation = 0;
```python=
while True:
    if situation == 1:
        #call API
    
    #è£½ä½œä¸€å€‹test.py
    #åŸ·è¡Œtest.py
    #æŽ¥æ”¶å›žå‚³çµæžœ
```
#### å·¥å…·
```python=
# è¨­å®šè¦åŸ·è¡Œçš„Pythonè…³æœ¬æ–‡ä»¶
command = "python try.txt"

# ä½¿ç”¨subprocessåŸ·è¡Œå‘½ä»¤
process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

# ç­‰å¾…å‘½ä»¤å®Œæˆ
stdout, stderr = process.communicate()

# æª¢æŸ¥å‘½ä»¤æ˜¯å¦æˆåŠŸ
if process.returncode == 0:
    print("å‘½ä»¤æˆåŠŸåŸ·è¡Œï¼Œè¼¸å‡ºç‚ºï¼š")
    print(stdout)
else:
    print("å‘½ä»¤åŸ·è¡Œå¤±æ•—ï¼ŒéŒ¯èª¤è¨Šæ¯ç‚ºï¼š")
    print(stderr)

```
å¯«å…¥GPTè¼¸å‡ºè‡³txt
```python=
user_result = '''
#python 
from bs4 import BeautifulSoup as bs
import requests

url="https://www.ptt.cc/bbs/Stock/index6651.html"
headers={
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
}

res = requests.get(url , headers=headers)
print(res.text)'''
path = 'python try'
f = open(path, 'w')
f.write(str(user_result))
f.close()
```
### æœ€çµ‚ç¨‹å¼ç¢¼
https://github.com/Hsing-Tzu/LLMLab/blob/main/LLMLAB_1101.ipynb
```python
user_result = "Hello World"
user_question = "1Can you give me the python code which can print Hello World?"

# prompt = ""+ user_question + "" + "ï¼Œä¸”é æœŸçµæžœç‚º" + user_result
```
```python
import requests
import subprocess

def CALL_API (user_question):
    
    API_URL = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1"
    headers = {"Authorization": "Bearer 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"}

    def query(payload):
        response = requests.post(API_URL, headers=headers, json=payload)
        return response.json()

    API_fetch = query({
        "inputs": user_question,
    })

    return API_fetch

API_fetch = CALL_API(user_question)
print(API_fetch)
```
```python
import re

def json_processed (API_fetch):
    text = API_fetch[0]['generated_text']
    code_blocks = re.findall(r'```\n(.*?)\n```', text, re.DOTALL)

    
    return block

json_processed = json_processed(API_fetch)
```
```python
situation = 1 #0æˆåŠŸã€1å¤±æ•—
error = 0
```
```python
while situation == 1:
    if error == 1:
        new_API_fetch = CALL_API(stderr)
        print(new_API_fetch)
        
    #è£½ä½œä¸€å€‹test.txt
    path = 'test.txt'
    f = open(path, 'w')
    f.write(str(json_processed))
    f.close()
    
    #åŸ·è¡Œtest.txt
    # è¨­å®šè¦åŸ·è¡Œçš„Pythonè…³æœ¬æ–‡ä»¶
    command = "python test.txt"

    # ä½¿ç”¨subprocessåŸ·è¡Œå‘½ä»¤
    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

    # ç­‰å¾…å‘½ä»¤å®Œæˆ
    stdout, stderr = process.communicate()
    stdout = stdout.strip()
    #print(stdout)

    
    # æª¢æŸ¥å‘½ä»¤æ˜¯å¦æˆåŠŸ
    if process.returncode == 0 and stdout == user_result:
        print("å‘½ä»¤æˆåŠŸåŸ·è¡Œï¼Œè¼¸å‡ºç‚ºï¼š")
        situation = 0
        print(stdout)
    else:
        print("å‘½ä»¤åŸ·è¡Œå¤±æ•—ï¼ŒéŒ¯èª¤è¨Šæ¯ç‚ºï¼š")
        situation = 1
        error = 1
        stderr = stderr + "This is not the result I want, the result I want is " + user_result + ". Can you generate a new code?"
        print(stderr)
```
## 11/6ï½œPre-Discussion
https://colab.research.google.com/drive/1ZveGLd_uMhTei-EAtOuOpsnPVgh7Bu3k?usp=sharing
### Fine-tuning
#### Upload a training file
```python
import os
import openai
openai.api_key = os.getenv("OPENAI_API_KEY")
openai.File.create(
  file=open("mydata.jsonl", "rb"),
  purpose='fine-tune'
)
```
### Create a fine-tuned model
```python
openai.FineTuningJob.create(training_file="file-abc123", model="gpt-3.5-turbo")
```
### Fine-tuning GPT-2
#### Test in huggingface
```python
#gpt-2
import requests

API_URL = "https://api-inference.huggingface.co/models/gpt2"
headers = {"Authorization": "Bearer {API_KEY}"}

def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({
	"inputs": "Can you generate a python code to make a bar chart based on matplotlib?",
})
```
#### Test in Colab
```python
pip install transformers
```
```python
pip install torch
```
```python
from google.colab import drive
drive.mount('/content/drive')
f = open('/content/drive/MyDrive/Colab Notebooks/test.txt','r')
a = f.read()
print(a)
f.close()
```
```python
import accelerate
from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments

# åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹å’Œåˆ†è¯å™¨
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# å‡†å¤‡æ•°æ®é›†
train_path = '/content/drive/MyDrive/Colab Notebooks/train.txt'
test_path = '/content/drive/MyDrive/Colab Notebooks/test.txt'

train_dataset = TextDataset(
  tokenizer=tokenizer,
  file_path=train_path,
  block_size=128
)

test_dataset = TextDataset(
  tokenizer=tokenizer,
  file_path=test_path,
  block_size=128
)

data_collator = DataCollatorForLanguageModeling(
  tokenizer=tokenizer, mlm=False
)

# å®šä¹‰è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./gpt2-finetuned",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    eval_steps=400,
    save_steps=800,
    warmup_steps=500,
    prediction_loss_only=True,
)

# åˆå§‹åŒ–è®­ç»ƒå™¨å¹¶å¼€å§‹å¾®è°ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

trainer.train()

```
## 11/7ï½œMeeting with Dr.Tsai
### word2vec
https://www.tensorflow.org/text/tutorials/word2vec
https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/word2vec.ipynb

### BERT
https://mofanpy.com/tutorials/machine-learning/nlp/cbow

### æ–¹å‘ç¢ºèª
æ‡‰è©²å…ˆembedding å†ä¾†è€ƒæ…®fine-tuning

## 11/8ï½œEmbedding
https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/videos/semantic_search.ipynb
### å°‡å¥å­ç·¨ç¢¼æˆåµŒå…¥å‘é‡
:::spoiler å°‡å¥å­ç·¨ç¢¼æˆåµŒå…¥å‘é‡

```
ä½¿ç”¨é è¨“ç·´çš„èªžè¨€æ¨¡åž‹==å°‡å¥å­ç·¨ç¢¼æˆåµŒå…¥å‘é‡==
```

é€™è¡Œä»£ç¢¼å°Žå…¥äº†PyTorchåº«ï¼Œé€™æ˜¯ä¸€å€‹åœ¨Pythonä¸­å¯¦ç¾æ·±åº¦å­¸ç¿’çš„æµè¡Œåº«ã€‚

```python
import torch
```
å¾žtransformersåº«ä¸­å°Žå…¥AutoTokenizerå’ŒAutoModelã€‚é€™å€‹åº«æä¾›äº†è¨±å¤šé è¨“ç·´çš„æ¨¡åž‹ï¼Œç”¨æ–¼è‡ªç„¶èªžè¨€è™•ç†ä»»å‹™ã€‚
```python
from transformers import AutoTokenizer, AutoModel
```
å®šç¾©äº†ä¸€å€‹åˆ—è¡¨sentencesï¼ŒåŒ…å«ä¸‰å€‹==è¦è™•ç†çš„è‹±æ–‡å¥å­==ã€‚
```python
sentences = [
    "I took my dog for a walk",
    "Today is going to rain",
    "I took my cat for a walk",
]
```
è¨­ç½®ä¸€å€‹è®Šé‡model_ckptï¼Œå®ƒåŒ…å«äº†==é è¨“ç·´æ¨¡åž‹çš„åç¨±==ï¼Œç”¨æ–¼å¾ŒçºŒåŠ è¼‰æ¨¡åž‹å’Œåˆ†è©žå™¨ã€‚
```python
model_ckpt = "sentence-transformers/all-MiniLM-L6-v2"
```
ä½¿ç”¨model_ckptä¸­æŒ‡å®šçš„æ¨¡åž‹åç¨±å‰µå»ºä¸€å€‹åˆ†è©žå™¨å¯¦ä¾‹ï¼Œé€™å€‹åˆ†è©žå™¨å°‡ç”¨æ–¼å°‡å¥å­è½‰æ›æˆ==æ¨¡åž‹èƒ½ç†è§£çš„æ ¼å¼==ã€‚
```python
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
```
åŠ è¼‰èˆ‡åˆ†è©žå™¨ç›¸å°æ‡‰çš„èªžè¨€æ¨¡åž‹ï¼Œé€™å€‹æ¨¡åž‹å°‡ç”¨ä¾†==ç”ŸæˆåµŒå…¥å‘é‡==ã€‚
```python
model = AutoModel.from_pretrained(model_ckpt)
```
èª¿ç”¨åˆ†è©žå™¨å°‡å¥å­åˆ—è¡¨è½‰æ›æˆæ¨¡åž‹éœ€è¦çš„è¼¸å…¥æ ¼å¼ï¼Œä¸¦å°‡å®ƒå€‘==è½‰æ›æˆPyTorchå¼µé‡==ã€‚é€™è£¡é‚„è¨­ç½®äº†å¡«å……ï¼ˆä»¥ä½¿æ‰€æœ‰å¥å­é•·åº¦ä¸€è‡´ï¼‰å’Œæˆªæ–·ï¼ˆä»¥é¿å…è¶…é•·å¥å­ï¼‰ã€‚
```python
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors="pt")
```
åœ¨ä¸è¨ˆç®—æ¢¯åº¦çš„æƒ…æ³ä¸‹åŸ·è¡Œæ¨¡åž‹ï¼ˆé€™åœ¨æŽ¨è«–æ™‚ç¯€çœè¨ˆç®—è³‡æºï¼‰ï¼Œä¸¦å°‡ç·¨ç¢¼å¾Œçš„è¼¸å…¥æä¾›çµ¦æ¨¡åž‹ï¼Œ==å¾—åˆ°æ¨¡åž‹çš„è¼¸å‡º==ã€‚
```python
with torch.no_grad():
    model_output = model(**encoded_input)
```
å¾žæ¨¡åž‹è¼¸å‡ºä¸­å–å‡ºæœ€å¾Œä¸€å±¤éš±è—ç‹€æ…‹ï¼Œé€™ä»£è¡¨äº†==è¼¸å…¥å¥å­ä¸­æ¯å€‹tokençš„åµŒå…¥å‘é‡==ã€‚
```python
token_embeddings = model_output.last_hidden_state
```
æ‰“å°å‡ºtokenåµŒå…¥çš„å½¢ç‹€ï¼Œé€™æœƒé¡¯ç¤ºå‡º==å¼µé‡çš„ç¶­åº¦==ï¼Œé€šå¸¸æ˜¯ï¼ˆ==å¥å­æ•¸é‡, æ¯å¥è©±çš„tokenæ•¸é‡, åµŒå…¥ç¶­åº¦==ï¼‰ã€‚
é€™å‘Šè¨´æˆ‘å€‘æ¨¡åž‹ç”¢ç”Ÿäº†å¤šå°‘å€‹åµŒå…¥å‘é‡ï¼Œä»¥åŠæ¯å€‹å‘é‡çš„å¤§å°ã€‚
```python
print(f"Token embeddings shape: {token_embeddings.size()}")
```
Output: ```Token embeddings shape: torch.Size([3, 9, 384])```

:::
### ç”¢ç”Ÿå¥å­ç´šçš„åµŒå…¥ä¸¦æ¨™æº–åŒ–

:::spoiler ç”¢ç”Ÿå¥å­ç´šçš„åµŒå…¥ä¸¦æ¨™æº–åŒ–
```
å®šç¾©mean_poolingï¼šè™•ç†ç”±transformeræ¨¡åž‹ï¼ˆå¦‚BERTæˆ–ç›¸ä¼¼æ¨¡åž‹ï¼‰ç”Ÿæˆçš„tokenåµŒå…¥ï¼Œä¸¦ä¸”ç”¢ç”Ÿå¥å­ç´šçš„åµŒå…¥ã€‚

æŽ¥è‘—ï¼Œå®ƒæ¨™æº–åŒ–é€™äº›åµŒå…¥ä¸¦æ‰“å°å‡ºåµŒå…¥çš„å½¢ç‹€ã€‚
```

é€™è¡Œä»£ç¢¼==å°Žå…¥PyTorchæ·±åº¦å­¸ç¿’åº«ä¸­çš„functionalæ¨¡çµ„==ï¼Œé€šå¸¸ç°¡ç¨±ç‚ºFã€‚é€™å€‹æ¨¡çµ„åŒ…å«äº†ä¸€çµ„å‡½æ•¸ï¼Œé€™äº›å‡½æ•¸å¯ä»¥åœ¨==ä¸éœ€è¦å®šç¾©å®Œæ•´ç¥žç¶“ç¶²çµ¡å±¤çš„æƒ…æ³ä¸‹ï¼Œç›´æŽ¥å°æ•¸æ“šé€²è¡Œæ“ä½œ==ã€‚
```python
import torch.nn.functional as F
```
å®šç¾©ä¸€å€‹åç‚ºmean_poolingçš„å‡½æ•¸
```python
# å®ƒæŽ¥å—æ¨¡åž‹è¼¸å‡ºå’Œæ³¨æ„åŠ›é®ç½©ï¼ˆattention maskï¼‰ä½œç‚ºåƒæ•¸ã€‚
def mean_pooling(model_output, attention_mask):
    
    #å¾žæ¨¡åž‹è¼¸å‡ºä¸­æå–æœ€å¾Œä¸€å±¤éš±è—ç‹€æ…‹ï¼Œé€™åŒ…å«äº†å¥å­ä¸­æ¯å€‹tokençš„åµŒå…¥è¡¨ç¤ºã€‚
    token_embeddings = model_output.last_hidden_state
    
    #é€™è¡Œä»£ç¢¼å°‡æ³¨æ„åŠ›é®ç½©å±•é–‹åˆ°èˆ‡tokenåµŒå…¥ç›¸åŒçš„å°ºå¯¸ã€‚unsqueeze(-1)åœ¨æœ€å¾Œä¸€å€‹ç¶­åº¦ä¸Šå¢žåŠ ä¸€å€‹è»¸ï¼Œä½¿å¾—é®ç½©å¯ä»¥é€šéŽexpandæ–¹æ³•å»¶å±•åˆ°èˆ‡åµŒå…¥ç›¸åŒçš„å½¢ç‹€ã€‚
    input_mask_expanded = (
        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    )
    
    # å°‡æ“´å±•å¾Œçš„é®ç½©æ‡‰ç”¨æ–¼tokenåµŒå…¥ï¼Œç„¶å¾Œå°æ¯å€‹å¥å­é€²è¡Œæ±‚å’Œï¼Œå¾—åˆ°åŠ æ¬Šçš„tokenåµŒå…¥ç¸½å’Œã€‚é€™å€‹ç¸½å’Œé€šéŽæ¯å€‹å¥å­çš„éžé›¶tokenæ•¸é‡é€²è¡Œæ­¸ä¸€åŒ–ï¼Œä½¿ç”¨torch.clampä¾†é¿å…é™¤ä»¥é›¶çš„æƒ…æ³ï¼Œæœ€å°å€¼è¨­å®šç‚º1e-9ã€‚
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(
        input_mask_expanded.sum(1), min=1e-9
    )

```
é€™è¡Œä»£ç¢¼èª¿ç”¨mean_poolingå‡½æ•¸ï¼Œ==å°‡æ¨¡åž‹è¼¸å‡ºå’Œç·¨ç¢¼è¼¸å…¥ä¸­çš„æ³¨æ„åŠ›é®ç½©ä½œç‚ºåƒæ•¸å‚³å…¥==ï¼Œå¾—åˆ°==æ¯å€‹å¥å­çš„åµŒå…¥==ã€‚
```python
sentence_embeddings = mean_pooling(model_output, encoded_input["attention_mask"])
```
ä½¿ç”¨PyTorchçš„functional APIä¸­çš„normalizeå‡½æ•¸å°å¥å­åµŒå…¥é€²è¡Œ==L2æ­¸ä¸€åŒ–==ï¼Œé€™æœ‰åŠ©æ–¼åœ¨å¾ŒçºŒçš„ä»»å‹™ä¸­æ”¹å–„æ¨¡åž‹çš„æ€§èƒ½å’Œç©©å®šæ€§ã€‚
```python
# Normalize the embeddings
sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)
```
æœ€å¾Œï¼Œé€™è¡Œä»£ç¢¼æ‰“å°å‡º==ç¶“éŽæ­¸ä¸€åŒ–è™•ç†çš„å¥å­åµŒå…¥å‘é‡çš„å½¢ç‹€==ã€‚é€™æœƒé¡¯ç¤ºå¼µé‡çš„ç¶­åº¦ï¼Œé€šå¸¸æ˜¯ï¼ˆ==å¥å­æ•¸é‡, åµŒå…¥å‘é‡ç¶­åº¦==ï¼‰ã€‚
é€™å‘Šè¨´æˆ‘å€‘ç¶“éŽè™•ç†å¾Œå¾—åˆ°äº†==å¤šå°‘å€‹å¥å­åµŒå…¥ã€ä»¥åŠæ¯å€‹åµŒå…¥çš„å¤§å°==ã€‚
```python
print(f"Sentence embeddings shape: {sentence_embeddings.size()}")
```
Output: ```Sentence embeddings shape: torch.Size([3, 384])```
:::

### è¨ˆç®—ä¸€çµ„å¥å­åµŒå…¥ä¹‹é–“çš„é¤˜å¼¦ç›¸ä¼¼åº¦
:::spoiler è¨ˆç®—ä¸€çµ„å¥å­åµŒå…¥ä¹‹é–“çš„é¤˜å¼¦ç›¸ä¼¼åº¦
é€™è¡Œä»£ç¢¼å°Žå…¥äº†==NumPy==åº«ï¼Œé€™æ˜¯Pythonä¸­ç”¨æ–¼é€²è¡Œç§‘å­¸è¨ˆç®—çš„åŸºç¤Žåº«ï¼Œæä¾›äº†å¤§é‡çš„æ•¸å­¸å‡½æ•¸å’Œå¤šç¶­é™£åˆ—æ“ä½œã€‚
```python
import numpy as np
```
å¾žsklearn.metrics.pairwiseæ¨¡çµ„ä¸­å°Žå…¥cosine_similarityå‡½æ•¸ã€‚sklearnï¼ˆScikit-learnï¼‰æ˜¯ä¸€å€‹æä¾›è¨±å¤šå¸¸è¦‹==æ©Ÿå™¨å­¸ç¿’ç®—æ³•çš„Pythonåº«==ï¼Œcosine_similarityç”¨æ–¼è¨ˆç®—å‘é‡ä¹‹é–“çš„==é¤˜å¼¦ç›¸ä¼¼åº¦==ã€‚
```python
from sklearn.metrics.pairwise import cosine_similarity
```
é€™è¡Œä»£ç¢¼å°‡PyTorchå¼µé‡ä¸­çš„==å¥å­åµŒå…¥è½‰æ›æˆNumPyé™£åˆ—==ã€‚detach()æ–¹æ³•å°‡åµŒå…¥å¾žç•¶å‰è¨ˆç®—åœ–ä¸­åˆ†é›¢å‡ºä¾†ï¼Œé€™æ¨£å¯ä»¥é˜²æ­¢åœ¨è½‰æ›æˆNumPyé™£åˆ—æ™‚ç™¼ç”Ÿæ¢¯åº¦ä¿¡æ¯çš„éŒ¯èª¤å‚³éžã€‚
```python
sentence_embeddings = sentence_embeddings.detach().numpy()
```
åˆå§‹åŒ–ä¸€å€‹äºŒç¶­é™£åˆ—scoresï¼Œç”¨é›¶å¡«å……ï¼Œå…¶å½¢ç‹€ç”±å¥å­åµŒå…¥çš„æ•¸é‡æ±ºå®šã€‚
é€™å€‹é™£åˆ—å°‡ç”¨ä¾†==å„²å­˜æ‰€æœ‰å¥å­åµŒå…¥ä¹‹é–“çš„é¤˜å¼¦ç›¸ä¼¼åº¦åˆ†æ•¸==ã€‚
```python
scores = np.zeros((sentence_embeddings.shape[0], sentence_embeddings.shape[0]))
```
é–‹å§‹ä¸€å€‹å¾ªç’°ï¼Œ==å°æ¯å€‹å¥å­åµŒå…¥é€²è¡Œè¿­ä»£==ã€‚
```python!
for idx in range(sentence_embeddings.shape[0]):
    
    # å°æ–¼æ¯å€‹å¥å­åµŒå…¥ï¼Œè¨ˆç®—å®ƒèˆ‡æ‰€æœ‰å…¶ä»–å¥å­åµŒå…¥çš„é¤˜å¼¦ç›¸ä¼¼åº¦ã€‚
    # cosine_similarityå‡½æ•¸æŽ¥å—å…©å€‹åƒæ•¸ï¼šä¸€å€‹æ˜¯å–®å€‹å¥å­åµŒå…¥ï¼ˆéœ€è¦åŒ…è£åœ¨åˆ—è¡¨ä¸­ï¼Œå› ç‚ºcosine_similarityæœŸæœ›äºŒç¶­é™£åˆ—ï¼‰ï¼Œå¦ä¸€å€‹æ˜¯æ•´å€‹å¥å­åµŒå…¥é™£åˆ—ã€‚
    # å‡½æ•¸è¿”å›žä¸€å€‹ä¸€ç¶­é™£åˆ—ï¼ŒåŒ…å«äº†ç•¶å‰å¥å­åµŒå…¥èˆ‡é™£åˆ—ä¸­æ¯å€‹å¥å­åµŒå…¥çš„ç›¸ä¼¼åº¦åˆ†æ•¸ï¼Œé€™äº›åˆ†æ•¸è¢«è³¦å€¼çµ¦scoresé™£åˆ—ä¸­å°æ‡‰çš„è¡Œã€‚
    
    scores[idx, :] = cosine_similarity([sentence_embeddings[idx]], sentence_embeddings)[0]
```
:::
### ç²å–æ–‡æœ¬çš„åµŒå…¥å‘é‡
::: spoiler ç²å–æ–‡æœ¬çš„åµŒå…¥å‘é‡
```
é€™æ®µç¨‹å¼ç¢¼ä½¿ç”¨äº†datasetsåº«ä¾†åŠ è¼‰å’Œé è™•ç†SQuADï¼ˆStanford Question Answering Datasetï¼‰æ•¸æ“šé›†ï¼Œä¸¦ä¸”å®šç¾©äº†ä¸€å€‹å‡½æ•¸ä¾†ç²å–æ–‡æœ¬çš„åµŒå…¥å‘é‡ã€‚
```
é€™è¡Œä»£ç¢¼å°Žå…¥äº†datasetsåº«çš„load_datasetå‡½æ•¸ï¼Œé€™å€‹å‡½æ•¸ç”¨æ–¼åŠ è¼‰å’Œè™•ç†å…¬é–‹å¯ç”¨çš„æ•¸æ“šé›†ã€‚
```python
from datasets import load_dataset
```
åŠ è¼‰SQuADæ•¸æ“šé›†çš„é©—è­‰é›†éƒ¨åˆ†ï¼Œä½¿ç”¨ç¨®å­42é€²è¡Œéš¨æ©Ÿæ‰“äº‚ï¼Œç„¶å¾Œé¸æ“‡å‰100å€‹æ¨£æœ¬ã€‚shuffleæ˜¯ç‚ºäº†ç¢ºä¿é¸æ“‡çš„æ˜¯éš¨æ©Ÿçš„æ¨£æœ¬ï¼Œè€Œselectå‰‡æ˜¯å¾žæ‰“äº‚å¾Œçš„æ•¸æ“šé›†ä¸­é¸æ“‡ä¸€å€‹å­é›†ã€‚
```python
squad = load_dataset("squad", split="validation").shuffle(seed=42).select(range(100))
```
å®šç¾©äº†ä¸€å€‹å‡½æ•¸get_embeddingsï¼Œå®ƒæŽ¥å—ä¸€å€‹æ–‡æœ¬åˆ—è¡¨ä½œç‚ºåƒæ•¸ã€‚
```python
def get_embeddings(text_list):
    
    #ä½¿ç”¨å…ˆå‰æåˆ°çš„åˆ†è©žå™¨å°‡æ–‡æœ¬åˆ—è¡¨è½‰æ›ç‚ºæ¨¡åž‹éœ€è¦çš„æ ¼å¼ï¼Œé€²è¡Œå¡«å……å’Œæˆªæ–·ä»¥ä¿æŒä¸€è‡´é•·åº¦ï¼Œä¸¦å°‡è½‰æ›å¾Œçš„è¼¸å…¥è½‰æ›ç‚ºPyTorchå¼µé‡ã€‚
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    
    #é€™è¡Œä»£ç¢¼å°‡åˆ†è©žå™¨è¼¸å‡ºçš„å­—å…¸é€²è¡Œäº†è§£åŒ…ï¼Œå®ƒå¯¦éš›ä¸Šæ²’æœ‰æ”¹è®Šä»»ä½•å…§å®¹ï¼Œå¯èƒ½æ˜¯ç‚ºäº†æ¸…æ™°è¡¨ç¤ºè¼¸å…¥çš„çµæ§‹æˆ–æ˜¯ä¿®å¾©æŸäº›ç’°å¢ƒä¸‹çš„ç›¸å®¹æ€§å•é¡Œã€‚
    encoded_input = {k: v for k, v in encoded_input.items()}
    
    #åœ¨ç„¡éœ€è¨ˆç®—æ¢¯åº¦çš„ä¸Šä¸‹æ–‡ä¸­åŸ·è¡Œæ¨¡åž‹ï¼Œé€™æ˜¯æŽ¨è«–éšŽæ®µå¸¸è¦‹çš„åšæ³•ï¼Œå¯ä»¥ç¯€çœè¨˜æ†¶é«”å’Œè¨ˆç®—æ™‚é–“ã€‚
    with torch.no_grad():
        model_output = model(**encoded_input)
        
    #è¿”å›žä½¿ç”¨mean_poolingå‡½æ•¸è™•ç†çš„æ¨¡åž‹è¼¸å‡ºï¼Œè©²å‡½æ•¸ç”¢ç”Ÿå¥å­ç´šçš„åµŒå…¥ã€‚
    return mean_pooling(model_output, encoded_input["attention_mask"])

```
ä½¿ç”¨mapå‡½æ•¸éæ­·SQuADæ•¸æ“šé›†çš„æ¯ä¸€å€‹æ¨£æœ¬ï¼Œå°æ–¼æ¯å€‹æ¨£æœ¬çš„contextå­—æ®µï¼Œä½¿ç”¨get_embeddingså‡½æ•¸è¨ˆç®—åµŒå…¥å‘é‡ã€‚è¨ˆç®—å¾Œï¼Œå°‡åµŒå…¥å¾žPyTorchçš„CUDAå¼µé‡è½‰æ›åˆ°CPUå¼µé‡ï¼Œå†å°‡å…¶è½‰ç‚ºNumPyé™£åˆ—ï¼Œä¸¦å°‡çµæžœå­˜å„²åœ¨æ–°çš„å­—æ®µembeddingsä¸­ã€‚é€™å€‹éŽç¨‹å°‡ç‚ºSQuADæ•¸æ“šé›†çš„æ¯å€‹æ¨£æœ¬æ·»åŠ å°æ‡‰çš„åµŒå…¥å‘é‡ã€‚

```python
squad_with_embeddings = squad.map(
    lambda x: {"embeddings": get_embeddings(x["context"]).cpu().numpy()[0]}
)
```
:::
### Last
:::spoiler Last
```
é€™æ®µç¨‹å¼ç¢¼å°åŒ…å«åµŒå…¥å‘é‡çš„SQuADæ•¸æ“šé›†å»ºç«‹äº†ä¸€å€‹FAISSç´¢å¼•ï¼Œä»¥ä¾¿å¿«é€Ÿé€²è¡Œé«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢ï¼Œç„¶å¾Œå°ä¸€å€‹ç‰¹å®šçš„å•é¡Œé€²è¡Œç·¨ç¢¼ä¸¦æª¢ç´¢æœ€ç›¸é—œçš„æ•¸æ“šé›†æ¢ç›®ã€‚é€™æ®µç¨‹å¼ç¢¼å°åŒ…å«åµŒå…¥å‘é‡çš„SQuADæ•¸æ“šé›†å»ºç«‹äº†ä¸€å€‹FAISSç´¢å¼•ï¼Œä»¥ä¾¿å¿«é€Ÿé€²è¡Œé«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢ï¼Œç„¶å¾Œå°ä¸€å€‹ç‰¹å®šçš„å•é¡Œé€²è¡Œç·¨ç¢¼ä¸¦æª¢ç´¢æœ€ç›¸é—œçš„æ•¸æ“šé›†æ¢ç›®ã€‚
```
:::

### w3school python çˆ¬èŸ²
æš«æ™‚åªçˆ¬å‡ºå…¶ä¸­ä¸€é (https://www.w3schools.com/python/python_getstarted.asp)

```python
from bs4 import BeautifulSoup 
import requests
import re
import pandas as pd

url="https://www.w3schools.com/python/python_getstarted.asp"
headers={
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
}

res = requests.get(url, headers=headers)
soup = BeautifulSoup(res.text, "html.parser")
sample_t=[]
# æŠ“å–å…§å®¹
sample = soup.find_all("div", class_="w3-example")
container_div = soup.find("div", class_="w3-col l10 m12")
if container_div:
    p_elements = container_div.find_all(['p', 'li'])
    p_elements_t = "\n".join([content.get_text() for content in p_elements])
for div in sample:
    text = div.text
    index = text.find(">>>")
    if index != -1:
        text = text[index + 3:]

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŽ»é™¤æ¢è¡Œå’Œç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    sample_t.append(text)
print(p_elements_t)
print("-"*50)
sample_t
```

### embedding code
[colab URL](https://colab.research.google.com/drive/1tKJ6cyL9y6Y8Fbtatcp1PhTcA8RDPdYO?usp=sharing)*ä½¿ç”¨å¸«å¤§å¸³è™Ÿã€å»ºç«‹å‰¯æœ¬*

[Three little pigs](https://github.com/gbishop/samisays/blob/master/samisays/three%20little%20pigs.txt)
[Harry Potter](https://github.com/bobdeng/owlreader/blob/master/ERead/assets/books/Harry%20Potter%20and%20The%20Half-Blood%20Prince.txt)

### embedding adjust
åœ¨ Word2Vec æ¨¡åž‹ä¸­ï¼Œå„å€‹å¯èª¿æ•´è®Šé‡çš„ç¯„åœå’ŒåŠŸèƒ½å¦‚ä¸‹ï¼š

- `vector_size`ï¼š==è©žå‘é‡çš„ç¶­åº¦==ï¼Œå¯ä»¥æ˜¯ä»»ä½•æ­£æ•´æ•¸ã€‚
    - å¢žåŠ ç¶­åº¦å¯ä»¥æé«˜æ¨¡åž‹çš„è¤‡é›œåº¦å’Œå®¹é‡ï¼Œä½†ä¹Ÿå¯èƒ½å°Žè‡´éŽæ“¬åˆï¼Œç‰¹åˆ¥æ˜¯åœ¨è©žå½™é‡ä¸å¤§çš„æ•¸æ“šé›†ä¸Šã€‚é€šå¸¸ç¯„åœåœ¨ 50 åˆ° 300 ä¹‹é–“ã€‚

- `window`ï¼š==ä¸Šä¸‹æ–‡çª—å£å¤§å°==ï¼Œä¹Ÿæ˜¯æ­£æ•´æ•¸ã€‚çª—å£å¤§å°æ±ºå®šäº†åœ¨è€ƒæ…®ç›®æ¨™è©žçš„ä¸Šä¸‹æ–‡æ™‚è¦è€ƒæ…®çš„å‘¨åœè©žèªžçš„ç¯„åœã€‚
    - çª—å£å¤§å°è¼ƒå°å¯èƒ½æœƒè®“æ¨¡åž‹å­¸ç¿’åˆ°æ›´å¤šé—œæ–¼è©žèªžç‰¹å®šç”¨é€”çš„ä¿¡æ¯ï¼Œè€Œè¼ƒå¤§çš„çª—å£å‰‡æœ‰åŠ©æ–¼å­¸ç¿’è©žèªžçš„å»£æ³›ç”¨é€”ã€‚ä¸€èˆ¬ä¾†èªªï¼Œçª—å£å¤§å°è¨­ç½®åœ¨ 5 åˆ° 20 ä¹‹é–“ã€‚

- `min_count`ï¼š==è©žé »ä¸‹é™==ï¼Œæ±ºå®šäº†è©žå¿…é ˆå‡ºç¾çš„æœ€å°æ¬¡æ•¸æ‰èƒ½è¢«è€ƒæ…®é€²è©žå½™è¡¨ã€‚é€™æ˜¯ä¸€å€‹éžè² æ•´æ•¸ã€‚
    - å°‡æ­¤å€¼è¨­ç½®å¾—å¤ªä½Žå¯èƒ½æœƒå°Žè‡´è¨±å¤šç½•è¦‹è©žæ±¡æŸ“è©žå‘é‡ç©ºé–“ï¼Œè¨­ç½®å¾—å¤ªé«˜å¯èƒ½æœƒå¿½ç•¥æœ‰ç”¨çš„ä¿¡æ¯ã€‚é€šå¸¸è¨­ç½®ç‚º 1 åˆ° 100ã€‚

- `workers`ï¼š==é€²è¡Œè¨“ç·´çš„ç·šç¨‹æ•¸é‡==ï¼Œé€™å€‹æ•¸å­—æ‡‰è©²å’Œä½ çš„æ©Ÿå™¨çš„è™•ç†å™¨æ ¸å¿ƒæ•¸ç›¸ç¬¦ã€‚é€™æ˜¯ä¸€å€‹æ­£æ•´æ•¸ï¼Œå¦‚æžœä½ çš„æ©Ÿå™¨æœ‰å¤šå€‹æ ¸å¿ƒï¼Œå¢žåŠ é€™å€‹å€¼å¯ä»¥åŠ å¿«è¨“ç·´é€Ÿåº¦ã€‚
    - ç„¶è€Œï¼Œé€™ä¸¦ä¸æœƒå½±éŸ¿è¨“ç·´å¾Œæ¨¡åž‹çš„æ€§èƒ½æˆ–è³ªé‡ã€‚

é€™äº›è®Šé‡å¯ä»¥æ ¹æ“šå…·é«”çš„æ•¸æ“šé›†å’Œæ‡‰ç”¨å ´æ™¯é€²è¡Œèª¿æ•´ï¼Œä»¥ç²å¾—æœ€ä½³çš„æ¨¡åž‹è¡¨ç¾ã€‚ä¾‹å¦‚ï¼Œè¼ƒå°çš„æ•¸æ“šé›†å¯èƒ½éœ€è¦è¼ƒå°çš„ `vector_size` å’Œè¼ƒé«˜çš„ `min_count`ï¼Œè€Œè¼ƒå¤§çš„æ•¸æ“šé›†å‰‡å¯èƒ½éœ€è¦è¼ƒå¤§çš„ `vector_size` å’Œè¼ƒå°çš„ `min_count`ã€‚èª¿æ•´é€™äº›åƒæ•¸æ™‚ï¼Œå¯èƒ½éœ€è¦é€šéŽäº¤å‰é©—è­‰ç­‰æ–¹æ³•ä¾†æ‰¾åˆ°æœ€å„ªçš„åƒæ•¸çµ„åˆã€‚

## 11/13ï½œPre-Discussion
https://hackmd.io/@meebox/SJ7Um4Whs

### Selenium Web Crawler for embedding data
:::spoiler Code
https://github.com/knyliu/LATIA112-1/blob/main/LATIA_HW2
:::
### Scrapy for embedding data
:::spoiler Code
https://github.com/knyliu/LATIA112-1/blob/main/LATIA_HW2

Create a project in terminal:
```
scrapy startproject w3schools_scrapy
```
Start a project in terminal:
```
scrapy crawl w3schools
```
"w3schools" is depended upon for the name in the Python code.
:::
---
> [name=åŠ‰å½¥è°·]
> @mason45ok å¯«çš„ä¸€é çˆ¬èŸ²ï¼Œä¸ä½†åªèƒ½çˆ¬ä¸€é ã€å–®ä¸€é åˆçˆ¬ä¸å®Œæ•´ã€‚

> [name=mason45ok]
> æˆ‘å¾ˆæŠ±æ­‰ðŸ˜¢

:::warning
To Be Discussed:
1. æœ‰äº›è³‡æ–™ï¼ˆé é¢ï¼‰åœ¨çˆ¬èŸ²æ™‚æœƒè¢«è·³éŽ
    ä¾‹å¦‚ï¼špython try...except
2. æœ‰äº›é é¢åº•ä¸‹é‚„æœ‰æ›´å¤šé é¢
    ä¾‹å¦‚ï¼šPython Stringã€‚
3. æœ‰äº›é é¢çš„å…§å®¹æœƒé‡è¤‡å­˜åˆ°csv fileä¸­
4. å¾Œé¢çš„é æ•¸å³ä¾¿XPathæ˜¯æ­£ç¢ºçš„ï¼ˆåœ¨ç¶²é ä¸­å­˜åœ¨æ­¤Xpathï¼‰ï¼Œå»ä¹Ÿç„¡æ³•çˆ¬èŸ²ã€‚è¿´åœˆæœ‰å¯«æª¢æ¸¬æ©Ÿåˆ¶ï¼Œæœƒå ±éŒ¯ï¼šElement not found for Xpathã€‚
    ä¸ç¢ºå®šæ˜¯ä¸æ˜¯è¢«æ“‹ï¼Ÿä½†åˆæ„Ÿè¦ºä¸æ˜¯ã€‚
:::
### BERT
https://mofanpy.com/tutorials/machine-learning/nlp/bert
> [name=åŠ‰å½¥è°·]
> We can do it after Mason finishes the web crawler.

## 11/15ï½œBERT Model Trainning
è¦å¾žé ­é–‹å§‹è¨“ç·´ä¸€å€‹ç©ºçš„BERTæ¨¡åž‹ï¼Œæ‚¨éœ€è¦éµå¾ªä»¥ä¸‹æ­¥é©Ÿï¼š

1. **æ•¸æ“šæº–å‚™**ï¼šBERTçš„è¨“ç·´éœ€è¦å¤§é‡çš„æ–‡æœ¬æ•¸æ“šã€‚é€™äº›æ•¸æ“šé€šå¸¸æ˜¯æœªæ¨™è¨˜çš„ï¼Œä¾†è‡ªå„ç¨®ä¾†æºï¼Œä¾‹å¦‚æ›¸ç±ã€ç¶²ç«™å’Œæ–°èžæ–‡ç« ã€‚æ‚¨éœ€è¦æ”¶é›†ä¸¦é è™•ç†é€™äº›æ•¸æ“šï¼ŒåŒ…æ‹¬åˆ†è©žï¼ˆTokenizationï¼‰ã€åŽ»é™¤ç‰¹æ®Šå­—ç¬¦ã€çµ±ä¸€å¤§å°å¯«ç­‰ã€‚

2. **é¸æ“‡é è¨“ç·´ä»»å‹™**ï¼šBERTé€šå¸¸é€šéŽå…©ç¨®é è¨“ç·´ä»»å‹™ä¾†è¨“ç·´ï¼šæŽ©ç¢¼èªžè¨€æ¨¡åž‹ï¼ˆMasked Language Model, MLMï¼‰å’Œä¸‹ä¸€å¥é æ¸¬ï¼ˆNext Sentence Prediction, NSPï¼‰ã€‚åœ¨MLMä¸­ï¼Œéš¨æ©Ÿåœ°å¾žè¼¸å…¥å¥å­ä¸­é®è”½ä¸€äº›å–®è©žï¼Œç„¶å¾Œè®“æ¨¡åž‹é æ¸¬é€™äº›å–®è©žã€‚åœ¨NSPä¸­ï¼Œæ¨¡åž‹éœ€è¦é æ¸¬å…©å€‹å¥å­æ˜¯å¦é€£çºŒã€‚

3. **å»ºç«‹æ¨¡åž‹æž¶æ§‹**ï¼šBERTæ˜¯ä¸€å€‹åŸºæ–¼Transformerçš„æ¨¡åž‹ï¼Œå…·æœ‰å¤šå±¤è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰å±¤ã€‚æ‚¨éœ€è¦æ ¹æ“šéœ€è¦é¸æ“‡åˆé©çš„å±¤æ•¸ã€éš±è—å–®å…ƒæ•¸ã€é ­çš„æ•¸ç›®ç­‰åƒæ•¸ã€‚

4. **è¨­ç½®è¶…åƒæ•¸**ï¼šæ‚¨éœ€è¦é¸æ“‡é©åˆæ‚¨æ•¸æ“šå’Œç¡¬ä»¶çš„å­¸ç¿’çŽ‡ã€æ‰¹å¤§å°ï¼ˆBatch Sizeï¼‰ã€è¨“ç·´å‘¨æœŸï¼ˆEpochsï¼‰ç­‰ã€‚

5. **è¨“ç·´æ¨¡åž‹**ï¼šä½¿ç”¨æ‚¨çš„æ•¸æ“šå’Œè¨­ç½®ï¼Œé–‹å§‹è¨“ç·´éŽç¨‹ã€‚é€™é€šå¸¸éœ€è¦ä½¿ç”¨GPUæˆ–TPUç­‰é«˜æ€§èƒ½è¨ˆç®—è³‡æºã€‚

6. **è©•ä¼°å’Œèª¿æ•´**ï¼šåœ¨è¨“ç·´éŽç¨‹ä¸­å’Œä¹‹å¾Œï¼Œè©•ä¼°æ¨¡åž‹æ€§èƒ½ï¼Œä¸¦æ ¹æ“šéœ€è¦é€²è¡Œèª¿æ•´ã€‚é€™å¯èƒ½åŒ…æ‹¬èª¿æ•´è¶…åƒæ•¸ã€æ·»åŠ æ›´å¤šæ•¸æ“šæˆ–æ›´æ”¹é è™•ç†æ­¥é©Ÿã€‚

7. **å¾®èª¿**ï¼šä¸€æ—¦æ¨¡åž‹åœ¨é è¨“ç·´ä»»å‹™ä¸Šè¡¨ç¾è‰¯å¥½ï¼Œæ‚¨å¯ä»¥é€šéŽåœ¨ç‰¹å®šä»»å‹™ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æžã€å•ç­”ç­‰ï¼‰ä¸Šçš„é€²ä¸€æ­¥è¨“ç·´ä¾†å¾®èª¿å®ƒã€‚

è«‹æ³¨æ„ï¼Œè¨“ç·´BERTæ˜¯ä¸€å€‹è³‡æºå¯†é›†å’Œæ™‚é–“å¯†é›†çš„éŽç¨‹ï¼Œéœ€è¦å¤§é‡çš„è¨ˆç®—è³‡æºå’Œæ™‚é–“ã€‚å¦‚æžœæ‚¨æ²’æœ‰è¶³å¤ çš„è³‡æºï¼Œå¯ä»¥è€ƒæ…®ä½¿ç”¨ç¾æˆçš„é è¨“ç·´æ¨¡åž‹é€²è¡Œå¾®èª¿ï¼Œè€Œä¸æ˜¯å¾žé ­é–‹å§‹è¨“ç·´ã€‚

:::spoiler BERT Code
```python
from transformers import BertTokenizer, BertForMaskedLM, BertConfig
from transformers import DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments
import torch
from torch.utils.data import Dataset

class TextDataset(Dataset):
    def __init__(self, tokenizer, file_path, block_size=512):
        self.tokenizer = tokenizer
        self.block_size = block_size
        self.examples = []

        with open(file_path, encoding="utf-8") as f:
            for line in f:
                batch_encoding = tokenizer(line, add_special_tokens=True, truncation=True, max_length=self.block_size, return_token_type_ids=False)
                self.examples.append(batch_encoding.input_ids)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        return torch.tensor(self.examples[i])

# åˆå§‹åŒ–tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")  # å¦‚æžœæ‚¨çš„æ•°æ®æ˜¯ä¸­æ–‡

# åŠ è½½æ•°æ®é›†
file_path = 'harry_potter_1.txt'  # æ‚¨çš„æ•°æ®æ–‡ä»¶è·¯å¾„
dataset = TextDataset(tokenizer, file_path)

# åˆ›å»ºæ•°æ®collatorï¼Œç”¨äºŽåŠ¨æ€padding
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

# é…ç½®æ¨¡åž‹
config = BertConfig.from_pretrained("bert-base-uncased")  # æ ¹æ®æ‚¨çš„éœ€æ±‚é€‰æ‹©é€‚å½“çš„é¢„è®­ç»ƒæ¨¡åž‹
model = BertForMaskedLM(config)

# è®­ç»ƒå‚æ•°è®¾ç½®
training_args = TrainingArguments(
    output_dir="./bert_output",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
)

# åˆå§‹åŒ–Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

# å¼€å§‹è®­ç»ƒ
trainer.train()

```
:::

## 11/20ï½œPre-Discussion


|   |11/22| 11/29 | 12/6 |
| -------- | -------- | -------- |-------- |
| ä¸»è¦é€²åº¦     | Get the Model| Test the model     | PPT slides     |
| Note |      ||Group Name|
| Daisy     |Study how to test the model|      | Text     |
| Kenny     |Clean the python data 11/27 Morning|      | Text     |
| Mason     |Study code for trainning model and train the model for python |      | Text     |
| Jessica     |Study code for trainning model and train the model for python |      | Text     |

**11/22**
* Online Disscussion
    * ç’°å¢ƒset up 
    * Check the final code for trainning model
    * code for trainning the model
    * train harry potter
* Home
    * clean the python data *1 (Mon. Morning 11/27)
    * Study code for trainning model and train the model for python *2
        * Kenny CANNOT train the model because of his terrible MacBook Pro.
    * Study how to test the model *1
## 11/22ï½œç’°å¢ƒå»ºç½®
[tensorflowåƒè€ƒç¶²å€](https://discuss.tensorflow.org/t/tensorflow-not-detecting-gpu/16295/6)

**å¤§å®¶éƒ½è‡³å°‘æƒ³ä¸€å€‹Group Name**

|  | Group Name 1 | Group Name 2 |
| -------- | -------- | -------- |
| Daisy     |   MDKJ (Mega Dialog Knowledge Jockey)   |   |
| Kenny     |      |      |
| Mason     |      |      |
| Jessica   |CodePioneers(CP)    |      |
## 11/27ï½œPre-Disscussion
[Bert-å•ç­”](https://medium.com/analytics-vidhya/question-answering-system-with-bert-ebe1130f8def)  
[Bert-ä»‹ç´¹](https://github.com/IKMLab/course_material/blob/master/bert-huggingface.ipynb)

### Problems we need to solve
* How do we get the model?(Or do we really need the model?)
    * word2Vec - å¾žç©ºçš„Modelé–‹å§‹train
        * We don't have that much data
    * BERT - é€éŽå·²æœ‰çš„modelç¹¼çºŒä¸‹åŽ»train
        * But how about the code?
    * fine-tunning - å¾®èª¿
        * Not we want
* å°ˆé¡Œæ–¹å‘
    * åŽŸæœ¬æƒ³è§£æ±ºï¼šæå•å¾Œæœƒç™¼æ•£ï¼ˆä¸”å¯ä»¥æ ¹æ“šæƒ…å¢ƒå›žç­”ï¼‰
    * ç¾åœ¨çš„å•é¡Œï¼šå¯èƒ½æ²’è¾¦æ³•å¾—åˆ°æ­£ç¢ºå›žç­”ï¼ˆä¸èƒ½ç”Ÿæˆcodeï¼‰
        * é‚£æˆ‘å€‘éœ€è¦èª¿æ•´å°ˆé¡Œæ–¹å‘å—Žï¼Ÿ
        * å³ä¾¿æˆ‘å€‘ç”¨BERTæˆåŠŸæŠŠmodelç”Ÿå‡ºä¾†ï¼Œæˆ‘å€‘å¯èƒ½ä¹Ÿå¾—ä¸åˆ°æ­£ç¢ºå›žè¦†
            * ä¹Ÿä¸ä¸€å®šå¯ä»¥é€éŽæƒ…å¢ƒå•ç­”
        * How about ç”¨ GPT-2åŠ ä¸Špython dataåŽ»å¾®èª¿ï¼Ÿ
            * æˆ‘å€‘çš„å‰µæ–°ï¼Ÿæˆ–è¨±æ˜¯æˆ‘å€‘å¯ä»¥è‡ªæˆ‘æª¢æ¸¬å›žè¦†æ˜¯å¦æ˜¯ç”¨æˆ¶æ‰€å¸Œæœ›ï¼Œç›´åˆ°ç›¸åŒï¼Œæ‰å›žå‚³ç¨‹å¼ç¢¼çµ¦ç”¨æˆ¶

## 11/29ï½œBERT Code and PPT Discussion
19:00 - 20:00 Serch and study the code

21:00 - XX:00 Discuss the PPT and Group name

> [name=Daisy]
train from scratch BERT

[BERT_from_scratch](https://github.com/antonio-f/BERT_from_scratch/blob/main/BERT_from_scratch.ipynb)

[Finetune a Pre-trained Model](https://huggingface.co/docs/transformers/training)

:::spoiler finetuning with asking question
```python=
def get_answer_using_bert(question, reference_text):
  
  bert_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

  bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

  input_ids = bert_tokenizer.encode(question, reference_text)
  input_tokens = bert_tokenizer.convert_ids_to_tokens(input_ids)

  sep_location = input_ids.index(bert_tokenizer.sep_token_id)
  first_seg_len, second_seg_len = sep_location + 1, len(input_ids) - (sep_location + 1)
  seg_embedding = [0] * first_seg_len + [1] * second_seg_len

  model_scores = bert_model(torch.tensor([input_ids]), 
  token_type_ids=torch.tensor([seg_embedding]))
  ans_start_loc, ans_end_loc = torch.argmax(model_scores[0]), torch.argmax(model_scores[1])
  result = ' '.join(input_tokens[ans_start_loc:ans_end_loc + 1])

  result = result.replace('#', '')
  return result

reference_text = 'Mukesh Dhirubhai Ambani was born on 19 April 1957 in the British Crown colony of Aden (present-day Yemen) to Dhirubhai Ambani and Kokilaben Ambani. He has a younger brother Anil Ambani and two sisters, Nina Bhadrashyam Kothari and Dipti Dattaraj Salgaonkar. Ambani lived only briefly in Yemen, because his father decided to move back to India in 1958 to start a trading business that focused on spices and textiles. The latter was originally named Vimal but later changed to Only Vimal His family lived in a modest two-bedroom apartment in Bhuleshwar, Mumbai until the 1970s. The family financial status slightly improved when they moved to India but Ambani still lived in a communal society, used public transportation, and never received an allowance. Dhirubhai later purchased a 14-floor apartment block called Sea Wind in Colaba, where, until recently, Ambani and his brother lived with their families on different floors.'
question = 'What is the name of mukesh ambani brother?'

get_answer_using_bert(question, reference_text)
```
:::

### ä½¿ä¸Šè¿°çš„ PyTorch ç¨‹å¼ç¢¼ä½¿ç”¨ GPU é€²è¡Œè¨ˆç®—
è¦ä½¿ä¸Šè¿°çš„ PyTorch ç¨‹å¼ç¢¼ä½¿ç”¨ GPU é€²è¡Œè¨ˆç®—ï¼Œæ‚¨éœ€è¦ç¢ºä¿å¹¾ä»¶äº‹æƒ…ï¼š

1. **ç¢ºèª CUDA å¯ç”¨**: é¦–å…ˆï¼Œæ‚¨éœ€è¦æª¢æŸ¥ CUDA æ˜¯å¦åœ¨æ‚¨çš„ç³»çµ±ä¸Šå¯ç”¨ã€‚å¯ä»¥ä½¿ç”¨ PyTorch çš„ `torch.cuda.is_available()` å‡½æ•¸ä¾†é€²è¡Œé€™ä¸€æª¢æŸ¥ã€‚

2. **æŒ‡å®šè¨ˆç®—è£ç½®**: å¦‚æžœ CUDA å¯ç”¨ï¼Œæ‚¨éœ€è¦æŒ‡å®šä¸€å€‹è£ç½®ï¼ˆdeviceï¼‰ï¼Œå°‡å…¶è¨­ç‚º `"cuda"`ã€‚å¦å‰‡ï¼Œæ‚¨å¯ä»¥å›žé€€åˆ°ä½¿ç”¨ CPUã€‚

3. **å°‡æ¨¡åž‹å’Œæ•¸æ“šç§»è‡³ GPU**: æ‚¨éœ€è¦å°‡æ‚¨çš„æ¨¡åž‹å’Œæ‰€æœ‰çš„è¼¸å…¥æ•¸æ“šï¼ˆå¦‚ `input_ids`, `attention_mask`, `labels`ï¼‰ç§»åˆ° GPU ä¸Šã€‚

ä»¥ä¸‹æ˜¯å°æ‚¨çš„ç¨‹å¼ç¢¼é€²è¡Œé€™äº›ä¿®æ”¹çš„ç¯„ä¾‹ï¼š

```python
import torch
from torch import nn
from tqdm import tqdm

# æª¢æŸ¥ CUDA æ˜¯å¦å¯ç”¨ä¸¦è¨­ç½®è£ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ¨¡åž‹åˆå§‹åŒ–å’Œå„ªåŒ–å™¨è¨­ç½®
model = ...  # æ›¿æ›ç‚ºæ‚¨çš„æ¨¡åž‹
optim = ...  # æ›¿æ›ç‚ºæ‚¨çš„å„ªåŒ–å™¨
model.to(device)  # å°‡æ¨¡åž‹ç§»è‡³è¨­å®šçš„è£ç½®

epochs = 2

for epoch in range(epochs):
    # setup loop with TQDM and dataloader
    loop = tqdm(loader, leave=True)
    for batch in loop:
        # initialize calculated gradients (from prev step)
        optim.zero_grad()

        # pull all tensor batches required for training
        # ç¢ºä¿æ‰€æœ‰è¼¸å…¥æ•¸æ“šä¹Ÿç§»è‡³ GPU
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        # process
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)

        # extract loss
        loss = outputs.loss

        # calculate loss for every parameter that needs grad update
        loss.backward()

        # update parameters
        optim.step()

        # print relevant info to progress bar
        loop.set_description(f'Epoch {epoch}')
        loop.set_postfix(loss=loss.item())
```

è«‹ç¢ºä¿åœ¨ä½¿ç”¨ CUDA ä¹‹å‰ï¼Œæ‚¨çš„ç³»çµ±å·²ç¶“å®‰è£äº†é©ç•¶çš„ NVIDIA é©…å‹•ç¨‹å¼å’Œ CUDA å·¥å…·åŒ…ã€‚æ­¤å¤–ï¼Œæ‚¨çš„ PyTorch ç‰ˆæœ¬ä¹Ÿéœ€è¦æ”¯æŒ CUDAã€‚
![image](https://hackmd.io/_uploads/B1qd0jESa.png)

---
**ä»€éº¼æ˜¯PyTorch/tensor flow**
> ä¸€å€‹åˆ¥äººå¯«å¥½çš„æ±è¥¿å¯ä»¥æŠŠPythonå¯«çµ¦CPUç®—çš„ç¨‹å¼æ”¹æˆç”¨GPUè¨ˆç®—


**CUDA?**
> GPUæœ€å°çš„å–®ä½å«åšCUDAï¼Œæ˜¯ä¸€å€‹ç¡¬é«”å–®ä½ï¼Œé¡žä¼¼æ ¸å¿ƒçš„æ¦‚å¿µã€‚

**æ‰€ä»¥ç¸½è€Œè¨€ä¹‹è¦æŠŠè¨ˆç®—çš„æ–¹å¼æ”¹ç‚ºGPUï¼Œè©²å¦‚å’Œè¨­å®šï¼Ÿ**
>GPUè¦åœ¨ä»¥ä¸‹çš„ç¶²å€ä¸­æ‰å¯ä»¥æˆåŠŸé‹ä½œ > https://developer.nvidia.com/cuda-gpus
>
> è¦ç¢ºå®šCUDAç‰ˆæœ¬ã€Pytorchçš„ç‰ˆæœ¬ä¸€è‡´ã€‚
> 
> CUDNNè»Ÿé«”ç¢ºå®šä½ æ˜¯ä»€éº¼å–®ä½ï¼ˆç ”ç©¶å–®ä½ï¼‰
>
>ä¸‹è¼‰CUDAï¼ŒCUDNNè§£å£“ç¸®ï¼Œè£¡é¢çš„æ±è¥¿æ‹–é€²CUDAå°æ‡‰è³‡æ–™å¤¾ => ç’°å¢ƒè¨­å®šå®Œæˆ


---

|  | To Do |  |
| -------- | -------- | -------- |
| Daisy     |      |      |
| Kenny     |      |      |
| Mason     |   Teach Us All About GPU Computing    |      |
| Jessica   |      |      |

## PPT
* éšŠä¼ä»‹ç´¹
    * ä¸€å€‹äººæƒ³ä¸€å€‹è‡ªæˆ‘ä»‹ç´¹
        * æ‰®æ¼”ä»€éº¼è§’è‰²
    * åœ˜éšŠåç¨±
        * ç‚ºä»€éº¼å«é€™å€‹åœ˜å
        * åœ˜éšŠç†å¿µã€åœ˜éšŠä»‹ç´¹
* å‰µä½œç†å¿µ
    * ç‚ºä»€éº¼è¦åšé€™å€‹å°ˆé¡Œï¼Ÿ
        * æƒ³è§£æ±ºä»€éº¼ï¼Ÿ
        * ç›®æ¨™ç”¨é€”
        * ç›®æ¨™æ—ç¾¤
* æˆæžœèªªæ˜Ž
    * è‡ªå‹•æ¸¬è©¦(æ“·å–ç¨‹å¼ç¢¼åŽ»å•å•é¡Œ)
    * æ¨¡åž‹
* BERT vs Fine-tunning
* å­¸ç¿’ã€å¿ƒè·¯æ­·ç¨‹
    1. word2Vec
    2. BERT from Scratch
    3. Fine-tuning a Pre-trained Model
* ç¨‹å¼èªªæ˜Ž
    * è‡ªå‹•æ¸¬è©¦(æ“·å–ç¨‹å¼ç¢¼åŽ»å•å•é¡Œ)
    * æ¨¡åž‹
        * word2Vec
        * BERT from Scratch
        * Pre-train
        * Fine-tuning
* å…¶ä»–è£œå……
    * æœªä¾†æœƒåšåˆ°ä»€éº¼(GPT-2)

## 12/13ï½œFirst Discussion

:::spoiler BERT from scratchç¨‹å¼ç¢¼
```python=
#!/usr/bin/env python
# coding=utf-8
# Copyright 2020 The HuggingFace Team All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Fine-tuning the library models for masked language modeling (BERT, ALBERT, RoBERTa...) on a text file or a dataset.

Here is the full list of checkpoints on the hub that can be fine-tuned by this script:
https://huggingface.co/models?filter=fill-mask
"""
# You can also adapt this script on your own masked language modeling task. Pointers for this are left as comments.

import logging
import math
import os
import sys
import warnings
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional

import datasets
import evaluate
from datasets import load_dataset

import transformers
from transformers import (
    CONFIG_MAPPING,
    MODEL_FOR_MASKED_LM_MAPPING,
    AutoConfig,
    AutoModelForMaskedLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    is_torch_tpu_available,
    set_seed,
)
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import check_min_version, send_example_telemetry
from transformers.utils.versions import require_version


# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
check_min_version("4.36.0.dev0")

require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/language-modeling/requirements.txt")

logger = logging.getLogger(__name__)
MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.
    """

    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The model checkpoint for weights initialization. Don't set if you want to train a model from scratch."
            )
        },
    )
    model_type: Optional[str] = field(
        default=None,
        metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
    )
    config_overrides: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override some existing default config settings when a model is trained from scratch. Example: "
                "n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
            )
        },
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    token: str = field(
        default=None,
        metadata={
            "help": (
                "The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
                "generated when running `huggingface-cli login` (stored in `~/.huggingface`)."
            )
        },
    )
    use_auth_token: bool = field(
        default=None,
        metadata={
            "help": "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead."
        },
    )
    trust_remote_code: bool = field(
        default=False,
        metadata={
            "help": (
                "Whether or not to allow for custom models defined on the Hub in their own modeling files. This option"
                "should only be set to `True` for repositories you trust and in which you have read the code, as it will "
                "execute code present on the Hub on your local machine."
            )
        },
    )
    low_cpu_mem_usage: bool = field(
        default=False,
        metadata={
            "help": (
                "It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded. "
                "set True will benefit LLM loading time and RAM consumption."
            )
        },
    )

    def __post_init__(self):
        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
            raise ValueError(
                "--config_overrides can't be used in combination with --config_name or --model_name_or_path"
            )


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    dataset_name: Optional[str] = field(
        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
    )
    train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
    validation_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    validation_split_percentage: Optional[int] = field(
        default=5,
        metadata={
            "help": "The percentage of the train set used as validation set in case there's no validation split"
        },
    )
    max_seq_length: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "The maximum total input sequence length after tokenization. Sequences longer "
                "than this will be truncated."
            )
        },
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "The number of processes to use for the preprocessing."},
    )
    mlm_probability: float = field(
        default=0.15, metadata={"help": "Ratio of tokens to mask for masked language modeling loss"}
    )
    line_by_line: bool = field(
        default=False,
        metadata={"help": "Whether distinct lines of text in the dataset are to be handled as distinct sequences."},
    )
    pad_to_max_length: bool = field(
        default=False,
        metadata={
            "help": (
                "Whether to pad all samples to `max_seq_length`. "
                "If False, will pad the samples dynamically when batching to the maximum length in the batch."
            )
        },
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of training examples to this "
                "value if set."
            )
        },
    )
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
                "value if set."
            )
        },
    )
    streaming: bool = field(default=False, metadata={"help": "Enable streaming mode"})

    def __post_init__(self):
        if self.streaming:
            require_version("datasets>=2.0.0", "The streaming feature requires `datasets>=2.0.0`")

        if self.dataset_name is None and self.train_file is None and self.validation_file is None:
            raise ValueError("Need either a dataset name or a training/validation file.")
        else:
            if self.train_file is not None:
                extension = self.train_file.split(".")[-1]
                if extension not in ["csv", "json", "txt"]:
                    raise ValueError("`train_file` should be a csv, a json or a txt file.")
            if self.validation_file is not None:
                extension = self.validation_file.split(".")[-1]
                if extension not in ["csv", "json", "txt"]:
                    raise ValueError("`validation_file` should be a csv, a json or a txt file.")


def main():
    # See all possible arguments in src/transformers/training_args.py
    # or by passing the --help flag to this script.
    # We now keep distinct sets of args, for a cleaner separation of concerns.

    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
        # If we pass only one argument to the script and it's the path to a json file,
        # let's parse it to get our arguments.
        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    if model_args.use_auth_token is not None:
        warnings.warn(
            "The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.",
            FutureWarning,
        )
        if model_args.token is not None:
            raise ValueError("`token` and `use_auth_token` are both specified. Please set only the argument `token`.")
        model_args.token = model_args.use_auth_token

    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The
    # information sent is the one passed as arguments along with your Python/PyTorch versions.
    send_example_telemetry("run_mlm", model_args, data_args)

    # Setup logging
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )

    if training_args.should_log:
        # The default of training_args.log_level is passive, so we set log level at info here to have that default.
        transformers.utils.logging.set_verbosity_info()

    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()

    # Log on each process the small summary:
    logger.warning(
        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, "
        + f"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}"
    )
    # Set the verbosity to info of the Transformers logger (on main process only):
    logger.info(f"Training/evaluation parameters {training_args}")

    # Detecting last checkpoint.
    last_checkpoint = None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError(
                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
                "Use --overwrite_output_dir to overcome."
            )
        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
            logger.info(
                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
            )

    # Set seed before initializing model.
    set_seed(training_args.seed)

    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)
    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/
    # (the dataset will be downloaded automatically from the datasets Hub
    #
    # For CSV/JSON files, this script will use the column called 'text' or the first column. You can easily tweak this
    # behavior (see below)
    #
    # In distributed training, the load_dataset function guarantee that only one local process can concurrently
    # download the dataset.
    if data_args.dataset_name is not None:
        # Downloading and loading a dataset from the hub.
        raw_datasets = load_dataset(
            data_args.dataset_name,
            data_args.dataset_config_name,
            cache_dir=model_args.cache_dir,
            token=model_args.token,
            streaming=data_args.streaming,
        )
        if "validation" not in raw_datasets.keys():
            raw_datasets["validation"] = load_dataset(
                data_args.dataset_name,
                data_args.dataset_config_name,
                split=f"train[:{data_args.validation_split_percentage}%]",
                cache_dir=model_args.cache_dir,
                token=model_args.token,
                streaming=data_args.streaming,
            )
            raw_datasets["train"] = load_dataset(
                data_args.dataset_name,
                data_args.dataset_config_name,
                split=f"train[{data_args.validation_split_percentage}%:]",
                cache_dir=model_args.cache_dir,
                token=model_args.token,
                streaming=data_args.streaming,
            )
    else:
        data_files = {}
        if data_args.train_file is not None:
            data_files["train"] = data_args.train_file
            extension = data_args.train_file.split(".")[-1]
        if data_args.validation_file is not None:
            data_files["validation"] = data_args.validation_file
            extension = data_args.validation_file.split(".")[-1]
        if extension == "txt":
            extension = "text"
        raw_datasets = load_dataset(
            extension,
            data_files=data_files,
            cache_dir=model_args.cache_dir,
            token=model_args.token,
        )

        # If no validation data is there, validation_split_percentage will be used to divide the dataset.
        if "validation" not in raw_datasets.keys():
            raw_datasets["validation"] = load_dataset(
                extension,
                data_files=data_files,
                split=f"train[:{data_args.validation_split_percentage}%]",
                cache_dir=model_args.cache_dir,
                token=model_args.token,
            )
            raw_datasets["train"] = load_dataset(
                extension,
                data_files=data_files,
                split=f"train[{data_args.validation_split_percentage}%:]",
                cache_dir=model_args.cache_dir,
                token=model_args.token,
            )

    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at
    # https://huggingface.co/docs/datasets/loading_datasets.

    # Load pretrained model and tokenizer
    #
    # Distributed training:
    # The .from_pretrained methods guarantee that only one local process can concurrently
    # download model & vocab.
    config_kwargs = {
        "cache_dir": model_args.cache_dir,
        "revision": model_args.model_revision,
        "token": model_args.token,
        "trust_remote_code": model_args.trust_remote_code,
    }
    if model_args.config_name:
        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)
    elif model_args.model_name_or_path:
        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
    else:
        config = CONFIG_MAPPING[model_args.model_type]()
        logger.warning("You are instantiating a new config instance from scratch.")
        if model_args.config_overrides is not None:
            logger.info(f"Overriding config: {model_args.config_overrides}")
            config.update_from_string(model_args.config_overrides)
            logger.info(f"New config: {config}")

    tokenizer_kwargs = {
        "cache_dir": model_args.cache_dir,
        "use_fast": model_args.use_fast_tokenizer,
        "revision": model_args.model_revision,
        "token": model_args.token,
        "trust_remote_code": model_args.trust_remote_code,
    }
    if model_args.tokenizer_name:
        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
    elif model_args.model_name_or_path:
        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)
    else:
        raise ValueError(
            "You are instantiating a new tokenizer from scratch. This is not supported by this script. "
            "You can do it from another script, save it, and load it from here, using --tokenizer_name."
        )

    if model_args.model_name_or_path:
        model = AutoModelForMaskedLM.from_pretrained(
            model_args.model_name_or_path,
            from_tf=bool(".ckpt" in model_args.model_name_or_path),
            config=config,
            cache_dir=model_args.cache_dir,
            revision=model_args.model_revision,
            token=model_args.token,
            trust_remote_code=model_args.trust_remote_code,
            low_cpu_mem_usage=model_args.low_cpu_mem_usage,
        )
    else:
        logger.info("Training new model from scratch")
        model = AutoModelForMaskedLM.from_config(config, trust_remote_code=model_args.trust_remote_code)

    # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch
    # on a small vocab and want a smaller embedding size, remove this test.
    embedding_size = model.get_input_embeddings().weight.shape[0]
    if len(tokenizer) > embedding_size:
        model.resize_token_embeddings(len(tokenizer))

    # Preprocessing the datasets.
    # First we tokenize all the texts.
    if training_args.do_train:
        column_names = list(raw_datasets["train"].features)
    else:
        column_names = list(raw_datasets["validation"].features)
    text_column_name = "text" if "text" in column_names else column_names[0]

    if data_args.max_seq_length is None:
        max_seq_length = tokenizer.model_max_length
        if max_seq_length > 1024:
            logger.warning(
                "The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
                " of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
                " override this default with `--block_size xxx`."
            )
            max_seq_length = 1024
    else:
        if data_args.max_seq_length > tokenizer.model_max_length:
            logger.warning(
                f"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the "
                f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
            )
        max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)

    if data_args.line_by_line:
        # When using line_by_line, we just tokenize each nonempty line.
        padding = "max_length" if data_args.pad_to_max_length else False

        def tokenize_function(examples):
            # Remove empty lines
            examples[text_column_name] = [
                line for line in examples[text_column_name] if len(line) > 0 and not line.isspace()
            ]
            return tokenizer(
                examples[text_column_name],
                padding=padding,
                truncation=True,
                max_length=max_seq_length,
                # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it
                # receives the `special_tokens_mask`.
                return_special_tokens_mask=True,
            )

        with training_args.main_process_first(desc="dataset map tokenization"):
            if not data_args.streaming:
                tokenized_datasets = raw_datasets.map(
                    tokenize_function,
                    batched=True,
                    num_proc=data_args.preprocessing_num_workers,
                    remove_columns=[text_column_name],
                    load_from_cache_file=not data_args.overwrite_cache,
                    desc="Running tokenizer on dataset line_by_line",
                )
            else:
                tokenized_datasets = raw_datasets.map(
                    tokenize_function,
                    batched=True,
                    remove_columns=[text_column_name],
                )
    else:
        # Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.
        # We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more
        # efficient when it receives the `special_tokens_mask`.
        def tokenize_function(examples):
            return tokenizer(examples[text_column_name], return_special_tokens_mask=True)

        with training_args.main_process_first(desc="dataset map tokenization"):
            if not data_args.streaming:
                tokenized_datasets = raw_datasets.map(
                    tokenize_function,
                    batched=True,
                    num_proc=data_args.preprocessing_num_workers,
                    remove_columns=column_names,
                    load_from_cache_file=not data_args.overwrite_cache,
                    desc="Running tokenizer on every text in dataset",
                )
            else:
                tokenized_datasets = raw_datasets.map(
                    tokenize_function,
                    batched=True,
                    remove_columns=column_names,
                )

        # Main data processing function that will concatenate all texts from our dataset and generate chunks of
        # max_seq_length.
        def group_texts(examples):
            # Concatenate all texts.
            concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
            total_length = len(concatenated_examples[list(examples.keys())[0]])
            # We drop the small remainder, and if the total_length < max_seq_length  we exclude this batch and return an empty dict.
            # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.
            total_length = (total_length // max_seq_length) * max_seq_length
            # Split by chunks of max_len.
            result = {
                k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]
                for k, t in concatenated_examples.items()
            }
            return result

        # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a
        # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value
        # might be slower to preprocess.
        #
        # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:
        # https://huggingface.co/docs/datasets/process#map

        with training_args.main_process_first(desc="grouping texts together"):
            if not data_args.streaming:
                tokenized_datasets = tokenized_datasets.map(
                    group_texts,
                    batched=True,
                    num_proc=data_args.preprocessing_num_workers,
                    load_from_cache_file=not data_args.overwrite_cache,
                    desc=f"Grouping texts in chunks of {max_seq_length}",
                )
            else:
                tokenized_datasets = tokenized_datasets.map(
                    group_texts,
                    batched=True,
                )

    if training_args.do_train:
        if "train" not in tokenized_datasets:
            raise ValueError("--do_train requires a train dataset")
        train_dataset = tokenized_datasets["train"]
        if data_args.max_train_samples is not None:
            max_train_samples = min(len(train_dataset), data_args.max_train_samples)
            train_dataset = train_dataset.select(range(max_train_samples))

    if training_args.do_eval:
        if "validation" not in tokenized_datasets:
            raise ValueError("--do_eval requires a validation dataset")
        eval_dataset = tokenized_datasets["validation"]
        if data_args.max_eval_samples is not None:
            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
            eval_dataset = eval_dataset.select(range(max_eval_samples))

        def preprocess_logits_for_metrics(logits, labels):
            if isinstance(logits, tuple):
                # Depending on the model and config, logits may contain extra tensors,
                # like past_key_values, but logits always come first
                logits = logits[0]
            return logits.argmax(dim=-1)

        metric = evaluate.load("accuracy")

        def compute_metrics(eval_preds):
            preds, labels = eval_preds
            # preds have the same shape as the labels, after the argmax(-1) has been calculated
            # by preprocess_logits_for_metrics
            labels = labels.reshape(-1)
            preds = preds.reshape(-1)
            mask = labels != -100
            labels = labels[mask]
            preds = preds[mask]
            return metric.compute(predictions=preds, references=labels)

    # Data collator
    # This one will take care of randomly masking the tokens.
    pad_to_multiple_of_8 = data_args.line_by_line and training_args.fp16 and not data_args.pad_to_max_length
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm_probability=data_args.mlm_probability,
        pad_to_multiple_of=8 if pad_to_multiple_of_8 else None,
    )

    # Initialize our Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else None,
        eval_dataset=eval_dataset if training_args.do_eval else None,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,
        preprocess_logits_for_metrics=preprocess_logits_for_metrics
        if training_args.do_eval and not is_torch_tpu_available()
        else None,
    )

    # Training
    if training_args.do_train:
        checkpoint = None
        if training_args.resume_from_checkpoint is not None:
            checkpoint = training_args.resume_from_checkpoint
        elif last_checkpoint is not None:
            checkpoint = last_checkpoint
        train_result = trainer.train(resume_from_checkpoint=checkpoint)
        trainer.save_model()  # Saves the tokenizer too for easy upload
        metrics = train_result.metrics

        max_train_samples = (
            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
        )
        metrics["train_samples"] = min(max_train_samples, len(train_dataset))

        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()

    # Evaluation
    if training_args.do_eval:
        logger.info("*** Evaluate ***")

        metrics = trainer.evaluate()

        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
        metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))
        try:
            perplexity = math.exp(metrics["eval_loss"])
        except OverflowError:
            perplexity = float("inf")
        metrics["perplexity"] = perplexity

        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)

    kwargs = {"finetuned_from": model_args.model_name_or_path, "tasks": "fill-mask"}
    if data_args.dataset_name is not None:
        kwargs["dataset_tags"] = data_args.dataset_name
        if data_args.dataset_config_name is not None:
            kwargs["dataset_args"] = data_args.dataset_config_name
            kwargs["dataset"] = f"{data_args.dataset_name} {data_args.dataset_config_name}"
        else:
            kwargs["dataset"] = data_args.dataset_name

    if training_args.push_to_hub:
        trainer.push_to_hub(**kwargs)
    else:
        trainer.create_model_card(**kwargs)


def _mp_fn(index):
    # For xla_spawn (TPUs)
    main()


if __name__ == "__main__":
    main()
```
:::

! pip install transformers==4.36.0.dev0

ERROR: Ignored the following yanked versions: 4.14.0, 4.25.0
ERROR: Could not find a version that satisfies the requirement transformers==4.36.0.dev0 (from versions: 0.1, 2.0.0, 2.1.0, 2.1.1, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 2.9.1, 2.10.0, 2.11.0, 3.0.0, 3.0.1, 3.0.2, 3.1.0, 3.2.0, 3.3.0, 3.3.1, 3.4.0, 3.5.0, 3.5.1, 4.0.0rc1, 4.0.0, 4.0.1, 4.1.0, 4.1.1, 4.2.0, 4.2.1, 4.2.2, 4.3.0rc1, 4.3.0, 4.3.1, 4.3.2, 4.3.3, 4.4.0, 4.4.1, 4.4.2, 4.5.0, 4.5.1, 4.6.0, 4.6.1, 4.7.0, 4.8.0, 4.8.1, 4.8.2, 4.9.0, 4.9.1, 4.9.2, 4.10.0, 4.10.1, 4.10.2, 4.10.3, 4.11.0, 4.11.1, 4.11.2, 4.11.3, 4.12.0, 4.12.1, 4.12.2, 4.12.3, 4.12.4, 4.12.5, 4.13.0, 4.14.1, 4.15.0, 4.16.0, 4.16.1, 4.16.2, 4.17.0, 4.18.0, 4.19.0, 4.19.1, 4.19.2, 4.19.3, 4.19.4, 4.20.0, 4.20.1, 4.21.0, 4.21.1, 4.21.2, 4.21.3, 4.22.0, 4.22.1, 4.22.2, 4.23.0, 4.23.1, 4.24.0, 4.25.1, 4.26.0, 4.26.1, 4.27.0, 4.27.1, 4.27.2, 4.27.3, 4.27.4, 4.28.0, 4.28.1, 4.29.0, 4.29.1, 4.29.2, 4.30.0, 4.30.1, 4.30.2, 4.31.0, 4.32.0, 4.32.1, 4.33.0, 4.33.1, 4.33.2, 4.33.3, 4.34.0, 4.34.1, 4.35.0, 4.35.1, 4.35.2, 4.36.0)
ERROR: No matching distribution found for transformers==4.36.0.dev0

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
Untitled-1.ipynb Cell 9 line 2
      1 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
----> 2 check_min_version("4.36.0")
      4 require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/language-modeling/requirements.txt")
      6 logger = logging.getLogger(__name__)

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\utils\__init__.py:241, in check_min_version(min_version)
    239     error_message = f"This example requires a minimum version of {min_version},"
    240 error_message += f" but the version found is {__version__}.\n"
--> 241 raise ImportError(
    242     error_message
    243     + "Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other "
    244     "versions of HuggingFace Transformers."
    245 )

ImportError: This example requires a minimum version of 4.36.0, but the version found is 4.35.1.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.


how to draw bar chart in python

My grandmother is an illiterate farmer, and I hope to let her know that her vegetable sales are increasing every year. In 2019, she sold 100 kg, in 2020 she sold 120 kg, in 2021 she sold 140 kg, in 2022 she sold 160 kg, and in 2023 she sold 200 kg.

How to get the data from MySQL using Python?

## 12/26ï½œSprint Week Day 1
### æˆ‘å€‘é‚„ç¼ºä»€éº¼ï¼Ÿ
* BERT from scratch and result
* Fine-tunning code and result
    * æŠŠbookè¨˜åœ¨è…¦å­è£¡é¢
* RAG test with pdf file
    * è€ƒè©¦open book
* PPT

https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891

---

[BERT from Scratch Tutorial - å¾ˆæ¸…æ¥š](https://coaxsoft.com/blog/building-bert-with-pytorch-from-scratch)
```
[CLS] My cat mice likes to sleep and does not like [MASK] mice jerry
[SEP]
[CLS] jerry is treated as my pet too [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
```
https://huggingface.co/daryl149/llama-2-13b-chat-hf

### Fine-Tuning Code
#### Code
:::spoiler Code
```python=
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# llama-2-13b-chat-hf

# è¼‰å…¥é è¨“ç·´çš„æ¨¡åž‹å’Œåˆ†è©žå™¨
model_name = "LLaMA-model-name"  # æ›¿æ›ç‚ºå¯¦éš›çš„ LLaMA æ¨¡åž‹åç¨±
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# ç‚ºå¾®èª¿æº–å‚™ä½ çš„æ•¸æ“šé›†
# é€™æ‡‰è©²æ˜¯åŒ…å«ä½ å¸Œæœ›æ¨¡åž‹å­¸ç¿’çš„æ–‡æœ¬çš„æ•¸æ“šé›†
dataset = "path_to_your_dataset"  # æ›¿æ›ç‚ºä½ æ•¸æ“šé›†çš„è·¯å¾‘

# å®šç¾©ä¸€å€‹æ–¹æ³•ä¾†é è™•ç†æ•¸æ“š
def preprocess_data(data):
    # åœ¨é€™è£¡å¯¦ç¾é è™•ç†æ­¥é©Ÿï¼Œå¦‚åˆ†è©žç­‰
    pass

# é è™•ç†ä½ çš„æ•¸æ“šé›†
processed_dataset = preprocess_data(dataset)

# å®šç¾©ä½ çš„è¨“ç·´åƒæ•¸
training_args = {
    "num_train_epochs": 3,   # è¨“ç·´é€±æœŸæ•¸
    "per_device_train_batch_size": 8,  # æ¯å€‹è¨­å‚™çš„è¨“ç·´æ‰¹æ¬¡å¤§å°
    "save_steps": 10_000,    # æ¯10,000æ­¥ä¿å­˜ä¸€æ¬¡æª¢æŸ¥é»ž
    "weight_decay": 0.01,    # æ¬Šé‡è¡°æ¸›çš„å¼·åº¦
    # æ ¹æ“šéœ€è¦æ·»åŠ æ›´å¤šåƒæ•¸
}

# åˆå§‹åŒ– Trainer
from transformers import Trainer, TrainingArguments

training_arguments = TrainingArguments(**training_args)
trainer = Trainer(
    model=model,
    args=training_arguments,
    train_dataset=processed_dataset,
    # æ ¹æ“šéœ€è¦æ·»åŠ æ›´å¤šåƒæ•¸
)

# é–‹å§‹å¾®èª¿
trainer.train()

# ä¿å­˜å¾®èª¿å¾Œçš„æ¨¡åž‹
model.save_pretrained("path_to_save_your_model")
```
:::

```
your_project_folder/
â”‚
â”œâ”€â”€ model/                 # é è¨“ç·´æ¨¡åž‹æª”æ¡ˆ
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ vocab.json
â”‚   â””â”€â”€ merges.txt
â”‚
â”œâ”€â”€ data/                  # è¨“ç·´æ•¸æ“šé›†
â”‚   â””â”€â”€ your_dataset.txt
â”‚
â”œâ”€â”€ scripts/               # å¾®èª¿è…³æœ¬
â”‚   â””â”€â”€ fine_tune.py
â”‚
â”œâ”€â”€ requirements.txt       # ç’°å¢ƒé…ç½®æª”æ¡ˆ
â”‚
â””â”€â”€ [å…¶ä»–è¼”åŠ©è…³æœ¬]

```
#### å¦‚æžœä½ çš„è³‡æ–™åœ¨æœ¬æ©Ÿç«¯ï¼š
:::spoiler å¦‚æžœä½ çš„è³‡æ–™åœ¨æœ¬æ©Ÿç«¯
```python=
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# æŒ‡å®šæœ¬åœ°æ¨¡åž‹çš„è·¯å¾‘
local_model_path = "path_to_your_local_model"  # æ›¿æ›ç‚ºä½ æœ¬åœ°æ¨¡åž‹çš„è·¯å¾‘

# è¼‰å…¥æœ¬åœ°çš„æ¨¡åž‹å’Œåˆ†è©žå™¨
tokenizer = AutoTokenizer.from_pretrained(local_model_path)
model = AutoModelForCausalLM.from_pretrained(local_model_path)

# ç‚ºå¾®èª¿æº–å‚™ä½ çš„æ•¸æ“šé›†
# é€™æ‡‰è©²æ˜¯åŒ…å«ä½ å¸Œæœ›æ¨¡åž‹å­¸ç¿’çš„æ–‡æœ¬çš„æ•¸æ“šé›†
dataset = "path_to_your_dataset"  # æ›¿æ›ç‚ºä½ æ•¸æ“šé›†çš„è·¯å¾‘

# å®šç¾©ä¸€å€‹æ–¹æ³•ä¾†é è™•ç†æ•¸æ“š
def preprocess_data(data):
    # åœ¨é€™è£¡å¯¦ç¾é è™•ç†æ­¥é©Ÿï¼Œå¦‚åˆ†è©žç­‰
    pass

# é è™•ç†ä½ çš„æ•¸æ“šé›†
processed_dataset = preprocess_data(dataset)

# å®šç¾©ä½ çš„è¨“ç·´åƒæ•¸
training_args = {
    "num_train_epochs": 3,   # è¨“ç·´é€±æœŸæ•¸
    "per_device_train_batch_size": 8,  # æ¯å€‹è¨­å‚™çš„è¨“ç·´æ‰¹æ¬¡å¤§å°
    "save_steps": 10_000,    # æ¯10,000æ­¥ä¿å­˜ä¸€æ¬¡æª¢æŸ¥é»ž
    "weight_decay": 0.01,    # æ¬Šé‡è¡°æ¸›çš„å¼·åº¦
    # æ ¹æ“šéœ€è¦æ·»åŠ æ›´å¤šåƒæ•¸
}

# åˆå§‹åŒ– Trainer
from transformers import Trainer, TrainingArguments

training_arguments = TrainingArguments(**training_args)
trainer = Trainer(
    model=model,
    args=training_arguments,
    train_dataset=processed_dataset,
    # æ ¹æ“šéœ€è¦æ·»åŠ æ›´å¤šåƒæ•¸
)

# é–‹å§‹å¾®èª¿
trainer.train()

# ä¿å­˜å¾®èª¿å¾Œçš„æ¨¡åž‹
model.save_pretrained("path_to_save_your_model")
```
:::

#### Using GPU Code
:::spoiler Using GPU
```python=
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
import torch

# æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ CUDA (GPU)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# æŒ‡å®šæœ¬åœ°æ¨¡åž‹çš„è·¯å¾‘
local_model_path = "FT_1226/model"  # æ›¿æ›ç‚ºä½ æœ¬åœ°æ¨¡åž‹çš„è·¯å¾‘

# è¼‰å…¥æœ¬åœ°çš„æ¨¡åž‹å’Œåˆ†è©žå™¨
tokenizer = AutoTokenizer.from_pretrained(local_model_path)
model = AutoModelForCausalLM.from_pretrained(local_model_path)

# ç‚ºå¾®èª¿æº–å‚™ä½ çš„æ•¸æ“šé›†
# é€™æ‡‰è©²æ˜¯åŒ…å«ä½ å¸Œæœ›æ¨¡åž‹å­¸ç¿’çš„æ–‡æœ¬çš„æ•¸æ“šé›†
dataset = "FT_1226/data/pythonTrainingData_w3schools.txt"  # æ›¿æ›ç‚ºä½ æ•¸æ“šé›†çš„è·¯å¾‘

# å®šç¾©ä¸€å€‹æ–¹æ³•ä¾†é è™•ç†æ•¸æ“š
def preprocess_data(data):
    # åœ¨é€™è£¡å¯¦ç¾é è™•ç†æ­¥é©Ÿï¼Œå¦‚åˆ†è©žç­‰
    pass

# é è™•ç†ä½ çš„æ•¸æ“šé›†
processed_dataset = preprocess_data(dataset)

# å®šç¾©ä½ çš„è¨“ç·´åƒæ•¸
# å®šç¾©è¨“ç·´åƒæ•¸ï¼Œä¸¦åœ¨å…¶ä¸­æŒ‡å®šä½¿ç”¨ GPU
training_args = TrainingArguments(
    output_dir="FT_1226/model_fine_tuned",  # æ¨¡åž‹å’Œè¨“ç·´æ—¥èªŒçš„è¼¸å‡ºç›®éŒ„
    num_train_epochs=3,
    per_device_train_batch_size=8,
    save_steps=10_000,
    weight_decay=0.01,
    # æ ¹æ“šéœ€è¦æ·»åŠ æ›´å¤šåƒæ•¸
    device=device  # æŒ‡å®šä½¿ç”¨çš„è¨­å‚™
)

# åˆå§‹åŒ– Trainer
from transformers import Trainer, TrainingArguments

training_arguments = TrainingArguments(**training_args)
# åˆå§‹åŒ– Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=processed_dataset,
    # æ ¹æ“šéœ€è¦æ·»åŠ æ›´å¤šåƒæ•¸
)

# é–‹å§‹å¾®èª¿
trainer.train()

# ä¿å­˜å¾®èª¿å¾Œçš„æ¨¡åž‹
model.save_pretrained("FT_1226/model_fine_tuned")
```
:::
#### Error
:::spoiler Error - 1
HTTPError                                 Traceback (most recent call last)
File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\huggingface_hub\utils\_errors.py:270, in hf_raise_for_status(response, endpoint_name)
    269 try:
--> 270     response.raise_for_status()
    271 except HTTPError as e:

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\requests\models.py:1021, in Response.raise_for_status(self)
   1020 if http_error_msg:
-> 1021     raise HTTPError(http_error_msg, response=self)

HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/FT_1226/model/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

RepositoryNotFoundError                   Traceback (most recent call last)
File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\utils\hub.py:389, in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)
    387 try:
    388     # Load from URL or cache if already cached
--> 389     resolved_file = hf_hub_download(
    390         path_or_repo_id,
    391         filename,
    392         subfolder=None if len(subfolder) == 0 else subfolder,
    393         repo_type=repo_type,
    394         revision=revision,
...
    420         f"'https://huggingface.co/{path_or_repo_id}' for available revisions."
    421     ) from e

OSError: FT_1226/model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
:::
:::spoiler Solve The Error - 1 Code
```python=
import os

# æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
local_model_path = "FT_1226/model"  # ç¡®ä¿è¿™æ˜¯æ­£ç¡®çš„è·¯å¾„
if not os.path.exists(local_model_path):
    print(f"è·¯å¾„ä¸å­˜åœ¨: {local_model_path}")
else:
    print(f"æ‰¾åˆ°æ¨¡åž‹è·¯å¾„: {local_model_path}")
    # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶æ˜¯å¦åœ¨è·¯å¾„ä¸­
    required_files = ['pytorch_model.bin', 'config.json']
    missing_files = [f for f in required_files if not os.path.isfile(os.path.join(local_model_path, f))]
    if missing_files:
        print(f"ç¼ºå¤±æ–‡ä»¶: {missing_files}")
    else:
        print("æ‰€æœ‰å¿…è¦çš„æ¨¡åž‹æ–‡ä»¶éƒ½å·²æ‰¾åˆ°")
```
:::

:::spoiler Error - 2
HTTPError                                 Traceback (most recent call last)
File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\huggingface_hub\utils\_errors.py:270, in hf_raise_for_status(response, endpoint_name)
    269 try:
--> 270     response.raise_for_status()
    271 except HTTPError as e:

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\requests\models.py:1021, in Response.raise_for_status(self)
   1020 if http_error_msg:
-> 1021     raise HTTPError(http_error_msg, response=self)

HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/FT_1226/model/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

RepositoryNotFoundError                   Traceback (most recent call last)
File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\utils\hub.py:389, in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)
    387 try:
    388     # Load from URL or cache if already cached
--> 389     resolved_file = hf_hub_download(
    390         path_or_repo_id,
    391         filename,
    392         subfolder=None if len(subfolder) == 0 else subfolder,
    393         repo_type=repo_type,
    394         revision=revision,
...
    420         f"'https://huggingface.co/{path_or_repo_id}' for available revisions."
    421     ) from e

OSError: FT_1226/model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...  
:::

### Test at home
:::spoiler Error - 3
output:
C:\Users\mason\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tqdm\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
WARNING:tensorflow:From C:\Users\mason\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

Using device: cuda
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at FT_1226/model and are newly initialized: 
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.  

Error:  
TypeError                                 Traceback (most recent call last)
Cell In[1], line 29
     25 processed_dataset = preprocess_data(dataset)
     27 # å®šç¾©ä½ çš„è¨“ç·´åƒæ•¸
     28 # å®šç¾©è¨“ç·´åƒæ•¸ï¼Œä¸¦åœ¨å…¶ä¸­æŒ‡å®šä½¿ç”¨ GPU
---> 29 training_args = TrainingArguments(
     30     output_dir="FT_1226/model_fine_tuned",  # æ¨¡åž‹å’Œè¨“ç·´æ—¥èªŒçš„è¼¸å‡ºç›®éŒ„
     31     num_train_epochs=3,
     32     per_device_train_batch_size=8,
     33     save_steps=10_000,
     34     weight_decay=0.01,
     35     # æ ¹æ“šéœ€è¦æ·»åŠ æ›´å¤šåƒæ•¸
     36     device=device  # æŒ‡å®šä½¿ç”¨çš„è¨­å‚™
     37 )
     39 # åˆå§‹åŒ– Trainer
     40 from transformers import Trainer, TrainingArguments

TypeError: TrainingArguments.\__init__() got an unexpected keyword argument 'device'  

:::

:::spoiler Error - 4

KeyboardInterrupt                         Traceback (most recent call last)
Cell In[1], line 13
     11 # è¼‰å…¥æœ¬åœ°çš„æ¨¡åž‹å’Œåˆ†è©žå™¨
     12 tokenizer = AutoTokenizer.from_pretrained(local_model_path)
---> 13 model = AutoModelForCausalLM.from_pretrained(local_model_path)
     15 # ç‚ºå¾®èª¿æº–å‚™ä½ çš„æ•¸æ“šé›†
     16 # é€™æ‡‰è©²æ˜¯åŒ…å«ä½ å¸Œæœ›æ¨¡åž‹å­¸ç¿’çš„æ–‡æœ¬çš„æ•¸æ“šé›†
     17 dataset = "FT_1226/data/pythonTrainingData_w3schools.txt"  # æ›¿æ›ç‚ºä½ æ•¸æ“šé›†çš„è·¯å¾‘

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\auto\auto_factory.py:566, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    564 elif type(config) in cls._model_mapping.keys():
    565     model_class = _get_model_class(config, cls._model_mapping)
--> 566     return model_class.from_pretrained(
    567         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    568     )
    569 raise ValueError(
    570     f"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\n"
    571     f"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}."
    572 )

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\modeling_utils.py:3480, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)
   3471     if dtype_orig is not None:
   3472         torch.set_default_dtype(dtype_orig)
   3473     (
   3474         model,
   3475         missing_keys,
   3476         unexpected_keys,
   3477         mismatched_keys,
   3478         offload_index,
   3479         error_msgs,
-> 3480     ) = cls._load_pretrained_model(
   3481         model,
   3482         state_dict,
   3483         loaded_state_dict_keys,  # XXX: rename?
   3484         resolved_archive_file,
   3485         pretrained_model_name_or_path,
   3486         ignore_mismatched_sizes=ignore_mismatched_sizes,
   3487         sharded_metadata=sharded_metadata,
   3488         _fast_init=_fast_init,
   3489         low_cpu_mem_usage=low_cpu_mem_usage,
   3490         device_map=device_map,
   3491         offload_folder=offload_folder,
   3492         offload_state_dict=offload_state_dict,
   3493         dtype=torch_dtype,
   3494         is_quantized=(getattr(model, "quantization_method", None) == QuantizationMethod.BITS_AND_BYTES),
   3495         keep_in_fp32_modules=keep_in_fp32_modules,
   3496     )
   3498 model.is_loaded_in_4bit = load_in_4bit
   3499 model.is_loaded_in_8bit = load_in_8bit

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\modeling_utils.py:3734, in PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)
   3732     set_initialized_submodules(model, _loaded_keys)
   3733     # This will only initialize submodules that are not marked as initialized by the line above.
-> 3734     model.apply(model._initialize_weights)
   3736 # Set some modules to fp32 if any
   3737 if keep_in_fp32_modules is not None:

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\torch\nn\modules\module.py:897, in Module.apply(self, fn)
    862 r"""Applies ``fn`` recursively to every submodule (as returned by ``.children()``)
    863 as well as self. Typical use includes initializing the parameters of a model
    864 (see also :ref:`nn-init-doc`).
   (...)
    894 
    895 """
    896 for module in self.children():
--> 897     module.apply(fn)
    898 fn(self)
    899 return self

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\torch\nn\modules\module.py:897, in Module.apply(self, fn)
    862 r"""Applies ``fn`` recursively to every submodule (as returned by ``.children()``)
    863 as well as self. Typical use includes initializing the parameters of a model
    864 (see also :ref:`nn-init-doc`).
   (...)
    894 
    895 """
    896 for module in self.children():
--> 897     module.apply(fn)
    898 fn(self)
    899 return self

    [... skipping similar frames: Module.apply at line 897 (2 times)]

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\torch\nn\modules\module.py:897, in Module.apply(self, fn)
    862 r"""Applies ``fn`` recursively to every submodule (as returned by ``.children()``)
    863 as well as self. Typical use includes initializing the parameters of a model
    864 (see also :ref:`nn-init-doc`).
   (...)
    894 
    895 """
    896 for module in self.children():
--> 897     module.apply(fn)
    898 fn(self)
    899 return self

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\torch\nn\modules\module.py:898, in Module.apply(self, fn)
    896 for module in self.children():
    897     module.apply(fn)
--> 898 fn(self)
    899 return self

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\modeling_utils.py:1390, in PreTrainedModel._initialize_weights(self, module)
   1388 if getattr(module, "_is_hf_initialized", False):
   1389     return
-> 1390 self._init_weights(module)
   1391 module._is_hf_initialized = True

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\llama\modeling_llama.py:732, in LlamaPreTrainedModel._init_weights(self, module)
    730 std = self.config.initializer_range
    731 if isinstance(module, nn.Linear):
--> 732     module.weight.data.normal_(mean=0.0, std=std)
    733     if module.bias is not None:
    734         module.bias.data.zero_()

KeyboardInterrupt: 
:::spoiler Error - 4
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
Cell In[1], line 13
     11 # è¼‰å…¥æœ¬åœ°çš„æ¨¡åž‹å’Œåˆ†è©žå™¨
     12 tokenizer = AutoTokenizer.from_pretrained(local_model_path)
---> 13 model = AutoModelForCausalLM.from_pretrained(local_model_path)
     15 # ç‚ºå¾®èª¿æº–å‚™ä½ çš„æ•¸æ“šé›†
     16 # é€™æ‡‰è©²æ˜¯åŒ…å«ä½ å¸Œæœ›æ¨¡åž‹å­¸ç¿’çš„æ–‡æœ¬çš„æ•¸æ“šé›†
     17 dataset = "FT_1226/data/pythonTrainingData_w3schools.txt"  # æ›¿æ›ç‚ºä½ æ•¸æ“šé›†çš„è·¯å¾‘

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\auto\auto_factory.py:566, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    564 elif type(config) in cls._model_mapping.keys():
    565     model_class = _get_model_class(config, cls._model_mapping)
--> 566     return model_class.from_pretrained(
    567         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    568     )
    569 raise ValueError(
    570     f"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\n"
    571     f"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}."
    572 )

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\modeling_utils.py:3480, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)
   3471     if dtype_orig is not None:
   3472         torch.set_default_dtype(dtype_orig)
   3473     (
   3474         model,
   3475         missing_keys,
   3476         unexpected_keys,
   3477         mismatched_keys,
   3478         offload_index,
   3479         error_msgs,
-> 3480     ) = cls._load_pretrained_model(
   3481         model,
   3482         state_dict,
   3483         loaded_state_dict_keys,  # XXX: rename?
   3484         resolved_archive_file,
   3485         pretrained_model_name_or_path,
   3486         ignore_mismatched_sizes=ignore_mismatched_sizes,
   3487         sharded_metadata=sharded_metadata,
   3488         _fast_init=_fast_init,
   3489         low_cpu_mem_usage=low_cpu_mem_usage,
   3490         device_map=device_map,
   3491         offload_folder=offload_folder,
   3492         offload_state_dict=offload_state_dict,
   3493         dtype=torch_dtype,
   3494         is_quantized=(getattr(model, "quantization_method", None) == QuantizationMethod.BITS_AND_BYTES),
   3495         keep_in_fp32_modules=keep_in_fp32_modules,
   3496     )
   3498 model.is_loaded_in_4bit = load_in_4bit
   3499 model.is_loaded_in_8bit = load_in_8bit

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\modeling_utils.py:3734, in PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)
   3732     set_initialized_submodules(model, _loaded_keys)
   3733     # This will only initialize submodules that are not marked as initialized by the line above.
-> 3734     model.apply(model._initialize_weights)
   3736 # Set some modules to fp32 if any
   3737 if keep_in_fp32_modules is not None:

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\torch\nn\modules\module.py:897, in Module.apply(self, fn)
    862 r"""Applies ``fn`` recursively to every submodule (as returned by ``.children()``)
    863 as well as self. Typical use includes initializing the parameters of a model
    864 (see also :ref:`nn-init-doc`).
   (...)
    894 
    895 """
    896 for module in self.children():
--> 897     module.apply(fn)
    898 fn(self)
    899 return self

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\torch\nn\modules\module.py:897, in Module.apply(self, fn)
    862 r"""Applies ``fn`` recursively to every submodule (as returned by ``.children()``)
    863 as well as self. Typical use includes initializing the parameters of a model
    864 (see also :ref:`nn-init-doc`).
   (...)
    894 
    895 """
    896 for module in self.children():
--> 897     module.apply(fn)
    898 fn(self)
    899 return self

    [... skipping similar frames: Module.apply at line 897 (2 times)]

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\torch\nn\modules\module.py:897, in Module.apply(self, fn)
    862 r"""Applies ``fn`` recursively to every submodule (as returned by ``.children()``)
    863 as well as self. Typical use includes initializing the parameters of a model
    864 (see also :ref:`nn-init-doc`).
   (...)
    894 
    895 """
    896 for module in self.children():
--> 897     module.apply(fn)
    898 fn(self)
    899 return self

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\torch\nn\modules\module.py:898, in Module.apply(self, fn)
    896 for module in self.children():
    897     module.apply(fn)
--> 898 fn(self)
    899 return self

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\modeling_utils.py:1390, in PreTrainedModel._initialize_weights(self, module)
   1388 if getattr(module, "_is_hf_initialized", False):
   1389     return
-> 1390 self._init_weights(module)
   1391 module._is_hf_initialized = True

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\llama\modeling_llama.py:732, in LlamaPreTrainedModel._init_weights(self, module)
    730 std = self.config.initializer_range
    731 if isinstance(module, nn.Linear):
--> 732     module.weight.data.normal_(mean=0.0, std=std)
    733     if module.bias is not None:
    734         module.bias.data.zero_()

KeyboardInterrupt: 
:::

## 12/27ï½œSprint Week Day 2
### æ–¹å‘èª¿æ•´

My grandmother is an illiterate farmer, and I hope to let her know that her vegetable sales are increasing every year. In 2019, she sold 100 kg, in 2020 she sold 120 kg, in 2021 she sold 140 kg, in 2022 she sold 160 kg, and in 2023 she sold 200 kg.Can you give me the code to generate the chart which title name Vegetable Sales Over Time?